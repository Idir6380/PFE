{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ2EOJkpG3ju",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /venv/main/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /venv/main/lib/python3.12/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /venv/main/lib/python3.12/site-packages (from accelerate) (2.7.0+cu128)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /venv/main/lib/python3.12/site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3274,
     "status": "ok",
     "timestamp": 1747327682266,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "tmo_VJxyHjl-",
    "outputId": "b4218254-4924-4789-f438-ab3de423bdcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /venv/main/lib/python3.12/site-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /venv/main/lib/python3.12/site-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /venv/main/lib/python3.12/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (from optuna) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /venv/main/lib/python3.12/site-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.12/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /venv/main/lib/python3.12/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /venv/main/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /venv/main/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: greenlet>=1 in /venv/main/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /venv/main/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 188160,
     "status": "ok",
     "timestamp": 1747327870429,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "7CoLT0Bf6Ju1",
    "outputId": "f331ad55-cc34-4fab-b0fb-cc77605ea797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.12/site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: torchvision in /venv/main/lib/python3.12/site-packages (0.22.0+cu128)\n",
      "Requirement already satisfied: torchaudio in /venv/main/lib/python3.12/site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /venv/main/lib/python3.12/site-packages (from torch) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /venv/main/lib/python3.12/site-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /venv/main/lib/python3.12/site-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /venv/main/lib/python3.12/site-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /venv/main/lib/python3.12/site-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /venv/main/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /venv/main/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /venv/main/lib/python3.12/site-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /venv/main/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /venv/main/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7602,
     "status": "ok",
     "timestamp": 1747327878023,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "jDe3P1gP8E0H",
    "outputId": "96db79c5-08d3-4534-d1ad-8435e6b41ef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xformers in /venv/main/lib/python3.12/site-packages (0.0.30)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (from xformers) (2.1.2)\n",
      "Requirement already satisfied: torch==2.7.0 in /venv/main/lib/python3.12/site-packages (from xformers) (2.7.0+cu128)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch==2.7.0->xformers) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy4mjw_HG70I",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Drive, HuggingFace, Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1747327900785,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "M3rPcmMMJiF9",
    "outputId": "94192fca-03a9-4e58-a864-836ea86ea6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: './diffusers'\n",
      "/workspace/PFE/models/diffusers/examples/text_to_image\n"
     ]
    }
   ],
   "source": [
    "%cd ./diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1747327900969,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "S7ylrNAD8VLJ",
    "outputId": "e79ed4a1-11a1-4dd7-8e58-bf10da762118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.cff        MANIFEST.in    _typos.toml  \u001b[0m\u001b[01;34mexamples\u001b[0m/       \u001b[01;34msrc\u001b[0m/\n",
      "CODE_OF_CONDUCT.md  Makefile       \u001b[01;34mbenchmarks\u001b[0m/  pyproject.toml  \u001b[01;34mtests\u001b[0m/\n",
      "CONTRIBUTING.md     PHILOSOPHY.md  \u001b[01;34mdocker\u001b[0m/      \u001b[01;34mscripts\u001b[0m/        \u001b[01;34mutils\u001b[0m/\n",
      "LICENSE             README.md      \u001b[01;34mdocs\u001b[0m/        setup.py\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1747327901879,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "O46KTpyq8cX0",
    "outputId": "82b46f64-f90f-4e87-cce4-ce54a47f166d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/PFE/models/diffusers/examples/text_to_image\n"
     ]
    }
   ],
   "source": [
    "%cd examples/text_to_image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "bd57424576d2425185e34ab3464f7ee1",
      "3de95f399ca14a6b87596df4f19401fa",
      "ca1e5c8ddbee49b78af11138dc4c89f7",
      "ab0b32dcd5d64ac79370f8322a9dab28",
      "18f1b6809eb7462eb8992b0ef123ed54",
      "6ceca2f4addf4d6abe4c7c2e9cec59f7",
      "a06b890f9f944c77a9f3c885290495d0",
      "4801fdbb01e54152baf472fca2d45ca4",
      "63748aebd8224662b91987cf59a7d239",
      "b6a01add170f4a81a2fe824c4f1e3e7a",
      "cd56972823b2436aa56496e040746628",
      "87ef2ee51ab0490bae86566a5d429e2a",
      "728750cf0d6240d7b986a90004dbd31b",
      "ea9a99ce48ca40b5955ba6bf981b8904",
      "60af33f0b5b245ddbfdf1b6048dce0ce",
      "f2a12f9e346d4e6294df808c7a590e13",
      "30ec6de5c21f4d57b04352fa516c1894",
      "ab9a6d775efd4181ba57cbc887386b79",
      "e60e1220d27b4bc89c765736c18bbb6a",
      "404b03575c404e0caa036ed4ca71713a"
     ]
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1747327902627,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "StEFcq4U8uOt",
    "outputId": "3780a322-a66a-4835-9865-166333f81ddd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26744c3a1a2e4467940720615bd71b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747327914164,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "HOdqUN4O9Wol",
    "outputId": "9e8e28dc-bc14-4db5-df11-f0bbeb0d9954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "README_sdxl.md\n",
      "\u001b[0m\u001b[01;34moptuna-sd-model_bs8_lr0.0001_schedcosine_acc4_0\u001b[0m/\n",
      "\u001b[01;34moptuna-sd-model_bs8_lr5e-05_schedlinear_acc4_0\u001b[0m/\n",
      "plot_contour.html\n",
      "plot_optimization_history.html\n",
      "plot_parallel_coordinates.html\n",
      "plot_param_importance.html\n",
      "plot_slice.html\n",
      "requirements.txt\n",
      "requirements_flax.txt\n",
      "requirements_sdxl.txt\n",
      "test_text_to_image.py\n",
      "test_text_to_image_lora.py\n",
      "train_text_to_image.py\n",
      "train_text_to_image_flax.py\n",
      "train_text_to_image_lora.py\n",
      "train_text_to_image_lora_sdxl.py\n",
      "train_text_to_image_sdxl.py\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12837,
     "status": "ok",
     "timestamp": 1747327928302,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "tyRvcTCg82wL",
    "outputId": "9b03110f-5a1c-4bb2-94f1-4d09a1e4a862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    }
   ],
   "source": [
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16410,
     "status": "ok",
     "timestamp": 1747327944714,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "OoHtRaF3-4da",
    "outputId": "7024b60b-bdd6-4c22-f1d7-c2c27f0c12bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: diffusers 0.34.0.dev0\n",
      "Uninstalling diffusers-0.34.0.dev0:\n",
      "  Successfully uninstalled diffusers-0.34.0.dev0\n",
      "Collecting git+https://github.com/huggingface/diffusers.git\n",
      "  Cloning https://github.com/huggingface/diffusers.git to /tmp/pip-req-build-e3yyr69j\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers.git /tmp/pip-req-build-e3yyr69j\n",
      "  Resolved https://github.com/huggingface/diffusers.git to commit 9836f0e000cfd826a7a5099002253ed2becc13e0\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib_metadata in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (8.7.0)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (0.30.2)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (2.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: Pillow in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (11.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (4.13.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /venv/main/lib/python3.12/site-packages (from importlib_metadata->diffusers==0.34.0.dev0) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests->diffusers==0.34.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.12/site-packages (from requests->diffusers==0.34.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests->diffusers==0.34.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.12/site-packages (from requests->diffusers==0.34.0.dev0) (2025.4.26)\n",
      "Building wheels for collected packages: diffusers\n",
      "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for diffusers: filename=diffusers-0.34.0.dev0-py3-none-any.whl size=3687437 sha256=909400beb40b6b25ffd51f1bcc4393a29729e39f119a31a5d4715192fd0189f7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-c_zw_02u/wheels/23/0f/7d/f97813d265ed0e599a78d83afd4e1925740896ca79b46cccfd\n",
      "Successfully built diffusers\n",
      "Installing collected packages: diffusers\n",
      "Successfully installed diffusers-0.34.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Remove current diffusers\n",
    "!pip uninstall -y diffusers\n",
    "\n",
    "# Install latest diffusers from GitHub (source install)\n",
    "!pip install git+https://github.com/huggingface/diffusers.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4319,
     "status": "ok",
     "timestamp": 1747327949037,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "jZpTJqU-Mv7V",
    "outputId": "59f0c323-65f3-496c-d234-561ee45581df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /venv/main/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.12/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /venv/main/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /venv/main/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/main/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /venv/main/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /venv/main/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suuJD99yinFj"
   },
   "source": [
    "Les meilleurs modeles:\n",
    "\n",
    "- SD-1.5 \\\\\n",
    "- SD-2.1 \\\\\n",
    "- Realistic_Vision_V6.0 \\\\\n",
    "- SDXL\n",
    "- FLUX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rShfSeR2HYQ2"
   },
   "source": [
    "### REALVISION1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1138863,
     "status": "ok",
     "timestamp": 1746434784044,
     "user": {
      "displayName": "racim saal",
      "userId": "12929633502212961087"
     },
     "user_tz": -60
    },
    "id": "6NjOsrbxg34m",
    "outputId": "e96b9bd7-032f-4de0-e48f-33796d5fa24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-05-05 08:28:18.597767: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746433698.627617   12012 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746433698.637566   12012 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "05/05/2025 08:28:52 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "scheduler_config.json: 100% 548/548 [00:00<00:00, 2.68MB/s]\n",
      "{'rescale_betas_zero_snr', 'timestep_spacing', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "tokenizer_config.json: 100% 737/737 [00:00<00:00, 3.70MB/s]\n",
      "vocab.json: 100% 1.06M/1.06M [00:00<00:00, 4.73MB/s]\n",
      "merges.txt: 100% 525k/525k [00:00<00:00, 3.79MB/s]\n",
      "special_tokens_map.json: 100% 472/472 [00:00<00:00, 2.70MB/s]\n",
      "config.json: 100% 612/612 [00:00<00:00, 3.41MB/s]\n",
      "model.safetensors: 100% 492M/492M [01:14<00:00, 6.64MB/s]\n",
      "config.json: 100% 577/577 [00:00<00:00, 4.87MB/s]\n",
      "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:41<00:00, 8.04MB/s]\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "config.json: 100% 1.55k/1.55k [00:00<00:00, 9.27MB/s]\n",
      "diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [05:41<00:00, 10.1MB/s]\n",
      "{'reverse_transformer_layers_per_block', 'encoder_hid_dim_type', 'dropout', 'num_attention_heads', 'attention_type', 'addition_time_embed_dim', 'transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Resolving data files: 100% 101/101 [00:00<00:00, 89315.77it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "05/05/2025 08:37:04 - INFO - __main__ - ***** Running training *****\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Num examples = 100\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Num Epochs = 8\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Total optimization steps = 200\n",
      "Steps:  12% 25/200 [00:41<04:56,  1.69s/it, lr=9.62e-5, step_loss=0.194]\n",
      "model_index.json: 100% 609/609 [00:00<00:00, 3.42MB/s]\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:02<00:08,  2.03s/it]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  60% 3/5 [00:02<00:01,  1.77it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:03<00:00,  1.37it/s]\n",
      "05/05/2025 08:37:50 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  25% 50/200 [01:46<04:01,  1.61s/it, lr=8.54e-5, step_loss=0.287]  {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:02<00:10,  2.69s/it]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:04<00:00,  1.17it/s]\n",
      "05/05/2025 08:38:55 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  38% 75/200 [02:53<03:20,  1.60s/it, lr=6.91e-5, step_loss=0.088] {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:01<00:05,  1.28s/it]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  60% 3/5 [00:01<00:00,  2.69it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:01<00:00,  3.08it/s]\n",
      "05/05/2025 08:39:59 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  50% 100/200 [03:55<02:35,  1.55s/it, lr=5e-5, step_loss=0.00566]  {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:02,  1.65it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:00<00:00,  5.95it/s]\n",
      "05/05/2025 08:41:01 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  62% 125/200 [04:58<01:58,  1.58s/it, lr=3.09e-5, step_loss=0.00452]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:01,  3.26it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:00<00:00,  9.20it/s]\n",
      "05/05/2025 08:42:03 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  75% 150/200 [06:00<01:18,  1.57s/it, lr=1.46e-5, step_loss=0.0425] {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:01,  2.66it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  60% 3/5 [00:00<00:00,  7.35it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:00<00:00,  7.05it/s]\n",
      "05/05/2025 08:43:06 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  88% 175/200 [07:02<00:38,  1.56s/it, lr=3.81e-6, step_loss=0.00444]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:01,  3.52it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:00<00:00,  9.47it/s]\n",
      "05/05/2025 08:44:07 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps: 100% 200/200 [08:04<00:00,  1.64s/it, lr=0, step_loss=0.47]       {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:01,  2.35it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:00<00:00,  7.31it/s]\n",
      "05/05/2025 08:45:09 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Model weights saved in sd-tryon-realistic_vision-lora/pytorch_lora_weights.safetensors\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:01,  3.05it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  80% 4/5 [00:00<00:00,  7.28it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'encoder_hid_dim_type', 'dropout', 'num_attention_heads', 'attention_type', 'addition_time_embed_dim', 'transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:16<00:00,  3.25s/it]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "05/05/2025 08:45:49 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "\n",
      "README.md: 100% 1.19k/1.19k [00:00<00:00, 9.43MB/s]\n",
      "\n",
      "image_0.png:   0% 0.00/312k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "image_1.png:   0% 0.00/342k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "image_2.png:   0% 0.00/348k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "image_3.png:   0% 0.00/326k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "events.out.tfevents.1746434224.998770d6f100.12012.1:   0% 0.00/2.40k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 8 LFS files:   0% 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "image_0.png:   5% 16.4k/312k [00:00<00:02, 122kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "image_3.png:   5% 16.4k/326k [00:00<00:02, 130kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "image_2.png:   5% 16.4k/348k [00:00<00:02, 123kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "image_1.png:   5% 16.4k/342k [00:00<00:02, 119kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "events.out.tfevents.1746434224.998770d6f100.12012.1: 100% 2.40k/2.40k [00:00<00:00, 9.06kB/s]\n",
      "image_3.png: 100% 326k/326k [00:00<00:00, 1.03MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "events.out.tfevents.1746432588.998770d6f100.7012.0:   0% 0.00/9.95M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image_0.png: 100% 312k/312k [00:00<00:00, 651kB/s] \n",
      "image_1.png: 100% 342k/342k [00:00<00:00, 624kB/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "events.out.tfevents.1746434224.998770d6f100.12012.0:  89% 10.7M/12.0M [00:00<00:00, 96.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "image_2.png: 100% 348k/348k [00:00<00:00, 594kB/s] \n",
      "\n",
      "pytorch_lora_weights.safetensors:   0% 0.00/3.23M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 8 LFS files:  12% 1/8 [00:00<00:04,  1.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "events.out.tfevents.1746434224.998770d6f100.12012.0: 100% 12.0M/12.0M [00:00<00:00, 31.3MB/s]\n",
      "events.out.tfevents.1746432588.998770d6f100.7012.0: 100% 9.95M/9.95M [00:00<00:00, 13.1MB/s]\n",
      "pytorch_lora_weights.safetensors: 100% 3.23M/3.23M [00:00<00:00, 4.99MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 8 LFS files:  75% 6/8 [00:01<00:00,  5.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 8 LFS files: 100% 8/8 [00:01<00:00,  5.81it/s]\n",
      "Steps: 100% 200/200 [09:11<00:00,  2.76s/it, lr=0, step_loss=0.47]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train_text_to_image_lora.py \\\n",
    "  --pretrained_model_name_or_path=\"SG161222/Realistic_Vision_V5.1_noVAE\" \\\n",
    "  --train_data_dir=\"/content/drive/MyDrive/PFE/mini/mini/images\" --caption_column=\"text\" \\\n",
    "  --dataloader_num_workers=8 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --max_train_steps=200 \\\n",
    "  --max_grad_norm=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --num_train_epochs=2 --checkpointing_steps=500 \\\n",
    "  --learning_rate=1e-04 --lr_scheduler=\"cosine\" --lr_warmup_steps=0 \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --seed=1337 \\\n",
    "  --output_dir=\"sd-tryon-realistic_vision-lora\" \\\n",
    "  --validation_prompt=\"a slim white woman in thirties.\" \\\n",
    "  --push_to_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "47c33d1b613d4020bd95daf95614605c",
      "e972ff5985dc45e2b55c06a54614eb73",
      "24581c35eccb4fcaa24783a372c1369b",
      "0f98e3ac88de4c0a8562509fdddf2414",
      "b4f7c1eb7e714503997750d7491e09df",
      "f47947a165e3431faf026311014037ca",
      "5aaf113e6c274350927dc7ffba2fd29a",
      "8d83a0171e7242eabcd9b48ede179ab8",
      "5160ab7f0ea4475b88c3f22befad8c9f",
      "ce2c638c0b2c425f85992f4cb3ae7914",
      "6e213250555c4fae8e8b4e12e4ac528b",
      "c16c7cdda7d14befade3967c3c5fea1d",
      "c901c86260be47ad965ae46a7cddaad0",
      "995e0ef750e9461b9ad8c025aa498b83",
      "fd5a2c5a60cb4c52bce596930902fd18",
      "0ab709fc478c47ad9b4b32835c08b7f2",
      "486b60aed1e248a59089d373e09255c1",
      "0b6f75c73b4245e5ac011b79ff88e316",
      "5745d5f89277404996948f96880dd048",
      "83571e554b864995bab0f73ff6bf38e7",
      "ce1566c5373f45eb8ef29d39506fd191",
      "ea4b1c230fa94020bc9039b5d82f2c40"
     ]
    },
    "executionInfo": {
     "elapsed": 29233,
     "status": "ok",
     "timestamp": 1746435500176,
     "user": {
      "displayName": "racim saal",
      "userId": "12929633502212961087"
     },
     "user_tz": -60
    },
    "id": "hu8NbclcbW-Z",
    "outputId": "13aa6ad1-1dff-4af8-acef-9d8a6f90b110"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c33d1b613d4020bd95daf95614605c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16c7cdda7d14befade3967c3c5fea1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_path = \"/content/diffusers/examples/text_to_image/sd-tryon-realistic_vision-lora\"\n",
    "pipe = DiffusionPipeline.from_pretrained(\"SG161222/Realistic_Vision_V5.1_noVAE\", torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.load_lora_weights(model_path)\n",
    "\n",
    "prompt = \"A white average woman wearing white clothes in twenties. White background. full body image.\"\n",
    "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
    "image.save(\"/content/img.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciAxLWjMkF1C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7DL-5T6a7y2"
   },
   "source": [
    "## SDXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Mr7y8LHSG2gi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 15:51:02,431] A new study created in memory with name: sdxl_optuna_study_20250517_155102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 5e-05, 'train_batch_size': 16, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 4, 'snr_gamma': 5.0, 'rank': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 15:51:09 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'rescale_betas_zero_snr', 'variance_type', 'dynamic_thresholding_ratio', 'clip_sample_range', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'shift_factor', 'latents_mean', 'latents_std', 'mid_block_add_attention', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 15:51:18 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 15:51:18 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 15:51:18 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 15:51:18 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/17/2025 15:51:18 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/17/2025 15:51:18 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "05/17/2025 15:51:18 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [40:09<00:00,  4.63s/it, lr=4.27e-5, optuna_metric=0.0911, step_loss=0.13] Model weights saved in optuna-sd-model_bs16_lr5e-05_schedcosine_acc4_0/pytorch_lora_weights.safetensors\n",
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:01<00:06,  1.17s/it]\u001b[A{'rescale_betas_zero_snr', 'timestep_type', 'use_beta_sigmas', 'use_exponential_sigmas', 'final_sigmas_type', 'sigma_min', 'sigma_max'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:04<00:01,  1.08it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  86%|████████▌ | 6/7 [00:04<00:00,  1.32it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:04<00:00,  1.41it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [40:16<00:00,  4.77s/it, lr=4.27e-5, optuna_metric=0.0911, step_loss=0.13]\n",
      "[I 2025-05-17 16:31:36,044] Trial 0 finished with value: 0.09111547058514195 and parameters: {'learning_rate': 5e-05, 'train_batch_size': 16, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 4, 'snr_gamma': 5.0, 'rank': 4}. Best is trial 0 with value: 0.09111547058514195.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 0] optuna_metric: 0.09111547058514195\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 5e-05, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 16:31:43 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'dynamic_thresholding_ratio', 'variance_type', 'rescale_betas_zero_snr', 'thresholding', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "{'shift_factor', 'mid_block_add_attention', 'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'attention_type', 'dropout', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 16:31:51 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 16:31:51 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 16:31:51 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 16:31:51 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/17/2025 16:31:51 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/17/2025 16:31:51 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "05/17/2025 16:31:51 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [40:38<00:00,  4.67s/it, lr=3.75e-5, optuna_metric=0.0911, step_loss=0.13]  Model weights saved in optuna-sd-model_bs16_lr5e-05_schedlinear_acc4_1/pytorch_lora_weights.safetensors\n",
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:01,  4.62it/s]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:01<00:03,  1.31it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'attention_type', 'dropout', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:05<00:02,  1.11s/it]\u001b[A{'sigma_max', 'final_sigmas_type', 'sigma_min', 'timestep_type', 'use_exponential_sigmas', 'use_beta_sigmas', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [40:44<00:00,  4.82s/it, lr=3.75e-5, optuna_metric=0.0911, step_loss=0.13]\n",
      "[I 2025-05-17 17:12:38,173] Trial 1 finished with value: 0.09108180216308243 and parameters: {'learning_rate': 5e-05, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 8}. Best is trial 1 with value: 0.09108180216308243.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 1] optuna_metric: 0.09108180216308243\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 1e-06, 'train_batch_size': 16, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 17:12:45 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'thresholding', 'rescale_betas_zero_snr', 'clip_sample_range', 'dynamic_thresholding_ratio', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "{'latents_mean', 'latents_std', 'mid_block_add_attention', 'use_post_quant_conv', 'use_quant_conv', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'attention_type', 'reverse_transformer_layers_per_block', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 17:12:53 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 17:12:53 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 17:12:53 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 17:12:53 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/17/2025 17:12:53 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/17/2025 17:12:53 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "05/17/2025 17:12:53 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [40:11<00:00,  4.57s/it, lr=8.54e-7, optuna_metric=0.0951, step_loss=0.133] Model weights saved in optuna-sd-model_bs16_lr1e-06_schedcosine_acc4_2/pytorch_lora_weights.safetensors\n",
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A{'use_exponential_sigmas', 'sigma_min', 'final_sigmas_type', 'rescale_betas_zero_snr', 'sigma_max', 'timestep_type', 'use_beta_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00,  8.10it/s]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:01<00:02,  1.82it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'attention_type', 'reverse_transformer_layers_per_block', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:05<00:02,  1.24s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [40:18<00:00,  4.77s/it, lr=8.54e-7, optuna_metric=0.0951, step_loss=0.133]\n",
      "[I 2025-05-17 17:53:14,549] Trial 2 finished with value: 0.09512028872497438 and parameters: {'learning_rate': 1e-06, 'train_batch_size': 16, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 16}. Best is trial 1 with value: 0.09108180216308243.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 2] optuna_metric: 0.09512028872497438\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 0.0001, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 17:53:21 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'rescale_betas_zero_snr', 'thresholding', 'dynamic_thresholding_ratio', 'clip_sample_range', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "{'shift_factor', 'latents_mean', 'use_post_quant_conv', 'use_quant_conv', 'mid_block_add_attention', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'dropout', 'attention_type', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 17:53:31 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 17:53:31 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 17:53:31 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 17:53:31 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/17/2025 17:53:31 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "05/17/2025 17:53:31 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/17/2025 17:53:31 - INFO - __main__ -   Total optimization steps = 255\n",
      "Steps: 100%|██████████| 255/255 [39:36<00:00,  7.89s/it, lr=8.75e-5, optuna_metric=0.0904, step_loss=0.13]  Model weights saved in optuna-sd-model_bs16_lr0.0001_schedlinear_acc8_3/pytorch_lora_weights.safetensors\n",
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:01,  4.16it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "{'use_exponential_sigmas', 'sigma_min', 'use_beta_sigmas', 'rescale_betas_zero_snr', 'timestep_type', 'sigma_max', 'final_sigmas_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  86%|████████▌ | 6/7 [00:01<00:00,  4.07it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'dropout', 'attention_type', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\u001b[A\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 255/255 [39:42<00:00,  9.34s/it, lr=8.75e-5, optuna_metric=0.0904, step_loss=0.13]\n",
      "[I 2025-05-17 18:33:15,905] Trial 3 finished with value: 0.09035178066369554 and parameters: {'learning_rate': 0.0001, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 3] optuna_metric: 0.09035178066369554\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 5e-05, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 18:33:23 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'thresholding', 'dynamic_thresholding_ratio', 'clip_sample_range', 'rescale_betas_zero_snr', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_add_attention', 'use_post_quant_conv', 'latents_std', 'latents_mean', 'use_quant_conv', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 18:33:31 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 18:33:31 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 18:33:31 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 18:33:31 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/17/2025 18:33:31 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "05/17/2025 18:33:31 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/17/2025 18:33:31 - INFO - __main__ -   Total optimization steps = 255\n",
      "Steps: 100%|██████████| 255/255 [39:58<00:00,  7.90s/it, lr=4.37e-5, optuna_metric=0.0908, step_loss=0.13]  Model weights saved in optuna-sd-model_bs16_lr5e-05_schedlinear_acc8_4/pytorch_lora_weights.safetensors\n",
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00,  8.68it/s]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:01<00:02,  1.84it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  57%|█████▋    | 4/7 [00:05<00:05,  1.70s/it]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "{'sigma_min', 'timestep_type', 'final_sigmas_type', 'rescale_betas_zero_snr', 'use_beta_sigmas', 'use_exponential_sigmas', 'sigma_max'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 255/255 [40:05<00:00,  9.43s/it, lr=4.37e-5, optuna_metric=0.0908, step_loss=0.13]\n",
      "[I 2025-05-17 19:13:38,636] Trial 4 finished with value: 0.09081702458206564 and parameters: {'learning_rate': 5e-05, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 4] optuna_metric: 0.09081702458206564\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 5e-05, 'train_batch_size': 16, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 19:13:45 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'dynamic_thresholding_ratio', 'thresholding', 'variance_type', 'clip_sample_range', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_add_attention', 'latents_mean', 'use_post_quant_conv', 'use_quant_conv', 'latents_std', 'shift_factor'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'dropout', 'attention_type', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 19:13:54 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 19:13:54 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 19:13:54 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 19:13:54 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/17/2025 19:13:54 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/17/2025 19:13:54 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "05/17/2025 19:13:54 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [40:16<00:00,  4.62s/it, lr=4.27e-5, optuna_metric=0.0911, step_loss=0.13] Model weights saved in optuna-sd-model_bs16_lr5e-05_schedcosine_acc4_5/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:01,  4.20it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'dropout', 'attention_type', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:03<00:10,  2.15s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "{'use_beta_sigmas', 'sigma_max', 'use_exponential_sigmas', 'sigma_min', 'timestep_type', 'rescale_betas_zero_snr', 'final_sigmas_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  86%|████████▌ | 6/7 [00:04<00:00,  1.39it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:04<00:00,  1.41it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [40:23<00:00,  4.78s/it, lr=4.27e-5, optuna_metric=0.0911, step_loss=0.13]\n",
      "[I 2025-05-17 19:54:19,312] Trial 5 finished with value: 0.09111547512075621 and parameters: {'learning_rate': 5e-05, 'train_batch_size': 16, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 4}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 5] optuna_metric: 0.09111547512075621\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 19:54:26 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'rescale_betas_zero_snr', 'dynamic_thresholding_ratio', 'variance_type', 'clip_sample_range', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'use_quant_conv', 'shift_factor', 'latents_mean', 'mid_block_add_attention', 'use_post_quant_conv', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 19:54:34 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 19:54:34 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 19:54:34 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 19:54:34 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
      "05/17/2025 19:54:34 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "05/17/2025 19:54:34 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "05/17/2025 19:54:34 - INFO - __main__ -   Total optimization steps = 1014\n",
      "Steps: 100%|██████████| 1014/1014 [44:03<00:00,  2.40s/it, lr=8.54e-5, optuna_metric=0.0906, step_loss=0.0483]Model weights saved in optuna-sd-model_bs8_lr0.0001_schedcosine_acc4_6/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:03<00:22,  3.68s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "{'sigma_min', 'timestep_type', 'use_beta_sigmas', 'rescale_betas_zero_snr', 'final_sigmas_type', 'sigma_max', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  86%|████████▌ | 6/7 [00:03<00:00,  1.98it/s]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\u001b[A\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 1014/1014 [44:10<00:00,  2.61s/it, lr=8.54e-5, optuna_metric=0.0906, step_loss=0.0483]\n",
      "[I 2025-05-17 20:38:46,820] Trial 6 finished with value: 0.090620937047462 and parameters: {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 4}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 6] optuna_metric: 0.090620937047462\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 20:38:54 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'thresholding', 'variance_type', 'rescale_betas_zero_snr', 'clip_sample_range', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "{'latents_std', 'use_quant_conv', 'mid_block_add_attention', 'latents_mean', 'shift_factor', 'use_post_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'dropout', 'reverse_transformer_layers_per_block', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 20:39:03 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 20:39:03 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 20:39:03 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 20:39:03 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
      "05/17/2025 20:39:03 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "05/17/2025 20:39:03 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "05/17/2025 20:39:03 - INFO - __main__ -   Total optimization steps = 1014\n",
      "Steps: 100%|██████████| 1014/1014 [43:44<00:00,  2.39s/it, lr=7.5e-5, optuna_metric=0.0905, step_loss=0.0456]Model weights saved in optuna-sd-model_bs8_lr0.0001_schedlinear_acc4_7/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:01,  4.06it/s]\u001b[A{'timestep_type', 'rescale_betas_zero_snr', 'use_beta_sigmas', 'sigma_max', 'use_exponential_sigmas', 'final_sigmas_type', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:01<00:01,  2.04it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'dropout', 'reverse_transformer_layers_per_block', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\u001b[A\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 1014/1014 [43:51<00:00,  2.60s/it, lr=7.5e-5, optuna_metric=0.0905, step_loss=0.0456]\n",
      "[I 2025-05-17 21:22:57,561] Trial 7 finished with value: 0.0904615197023331 and parameters: {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 16}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 7] optuna_metric: 0.0904615197023331\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 1e-05, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4, 'snr_gamma': 5.0, 'rank': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 21:23:04 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'dynamic_thresholding_ratio', 'variance_type', 'rescale_betas_zero_snr', 'clip_sample_range', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'shift_factor', 'mid_block_add_attention', 'use_quant_conv', 'latents_std', 'use_post_quant_conv', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 21:23:13 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 21:23:13 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 21:23:13 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 21:23:13 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/17/2025 21:23:13 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/17/2025 21:23:13 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "05/17/2025 21:23:13 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [40:20<00:00,  4.63s/it, lr=7.5e-6, optuna_metric=0.0926, step_loss=0.13] Model weights saved in optuna-sd-model_bs16_lr1e-05_schedlinear_acc4_8/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:01<00:03,  1.64it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  57%|█████▋    | 4/7 [00:04<00:04,  1.34s/it]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:05<00:02,  1.01s/it]\u001b[A{'sigma_min', 'timestep_type', 'rescale_betas_zero_snr', 'final_sigmas_type', 'use_exponential_sigmas', 'sigma_max', 'use_beta_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [40:27<00:00,  4.79s/it, lr=7.5e-6, optuna_metric=0.0926, step_loss=0.13]\n",
      "[I 2025-05-17 22:03:42,731] Trial 8 finished with value: 0.09257420724785821 and parameters: {'learning_rate': 1e-05, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4, 'snr_gamma': 5.0, 'rank': 16}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 8] optuna_metric: 0.09257420724785821\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 5e-05, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 22:03:49 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'thresholding', 'variance_type', 'dynamic_thresholding_ratio', 'clip_sample_range', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "{'shift_factor', 'latents_mean', 'latents_std', 'mid_block_add_attention', 'use_quant_conv', 'use_post_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 22:03:58 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 22:03:58 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 22:03:58 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 22:03:58 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/17/2025 22:03:58 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/17/2025 22:03:58 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "05/17/2025 22:03:58 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [40:23<00:00,  4.63s/it, lr=3.75e-5, optuna_metric=0.0911, step_loss=0.129] Model weights saved in optuna-sd-model_bs16_lr5e-05_schedlinear_acc4_9/pytorch_lora_weights.safetensors\n",
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:00<00:01,  4.04it/s]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:01<00:01,  2.04it/s]\u001b[A{'timestep_type', 'final_sigmas_type', 'sigma_max', 'sigma_min', 'use_exponential_sigmas', 'rescale_betas_zero_snr', 'use_beta_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:05<00:02,  1.18s/it]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [40:30<00:00,  4.79s/it, lr=3.75e-5, optuna_metric=0.0911, step_loss=0.129]\n",
      "[I 2025-05-17 22:44:31,424] Trial 9 finished with value: 0.09112482629606397 and parameters: {'learning_rate': 5e-05, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4, 'snr_gamma': None, 'rank': 16}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 9] optuna_metric: 0.09112482629606397\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 22:44:38 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'rescale_betas_zero_snr', 'clip_sample_range', 'thresholding', 'dynamic_thresholding_ratio', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "{'shift_factor', 'use_quant_conv', 'use_post_quant_conv', 'latents_mean', 'mid_block_add_attention', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 22:44:46 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 22:44:46 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 22:44:46 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 22:44:46 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
      "05/17/2025 22:44:46 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/17/2025 22:44:46 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/17/2025 22:44:46 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [43:21<00:00,  4.91s/it, lr=8.75e-5, optuna_metric=0.0908, step_loss=0.0447]Model weights saved in optuna-sd-model_bs8_lr0.0001_schedlinear_acc8_10/pytorch_lora_weights.safetensors\n",
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:01<00:06,  1.15s/it]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:01<00:03,  1.66it/s]\u001b[A{'timestep_type', 'rescale_betas_zero_snr', 'use_exponential_sigmas', 'final_sigmas_type', 'use_beta_sigmas', 'sigma_min', 'sigma_max'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  57%|█████▋    | 4/7 [00:01<00:00,  3.45it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  86%|████████▌ | 6/7 [00:05<00:00,  1.03it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [43:27<00:00,  5.14s/it, lr=8.75e-5, optuna_metric=0.0908, step_loss=0.0447]\n",
      "[I 2025-05-17 23:28:16,668] Trial 10 finished with value: 0.09076598939952425 and parameters: {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 8}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 10] optuna_metric: 0.09076598939952425\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 23:28:23 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'clip_sample_range', 'rescale_betas_zero_snr', 'variance_type', 'dynamic_thresholding_ratio', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'latents_std', 'use_quant_conv', 'latents_mean', 'mid_block_add_attention', 'shift_factor', 'use_post_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 23:28:32 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 23:28:32 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 23:28:32 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 23:28:32 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
      "05/17/2025 23:28:32 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/17/2025 23:28:32 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/17/2025 23:28:32 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [43:46<00:00,  4.95s/it, lr=8.75e-5, optuna_metric=0.0906, step_loss=0.0457]Model weights saved in optuna-sd-model_bs8_lr0.0001_schedlinear_acc8_11/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:03<00:21,  3.66s/it]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  57%|█████▋    | 4/7 [00:03<00:02,  1.31it/s]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:05<00:01,  1.15it/s]\u001b[A{'rescale_betas_zero_snr', 'use_beta_sigmas', 'sigma_max', 'timestep_type', 'final_sigmas_type', 'sigma_min', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [43:52<00:00,  5.19s/it, lr=8.75e-5, optuna_metric=0.0906, step_loss=0.0457]\n",
      "[I 2025-05-18 00:12:27,088] Trial 11 finished with value: 0.09062700855956361 and parameters: {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 16}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 11] optuna_metric: 0.09062700855956361\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/18/2025 00:12:34 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'dynamic_thresholding_ratio', 'thresholding', 'clip_sample_range', 'variance_type', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "{'latents_std', 'use_post_quant_conv', 'use_quant_conv', 'shift_factor', 'mid_block_add_attention', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/18/2025 00:12:43 - INFO - __main__ - ***** Running training *****\n",
      "05/18/2025 00:12:43 - INFO - __main__ -   Num examples = 10805\n",
      "05/18/2025 00:12:43 - INFO - __main__ -   Num Epochs = 3\n",
      "05/18/2025 00:12:43 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
      "05/18/2025 00:12:43 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/18/2025 00:12:43 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/18/2025 00:12:43 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [43:34<00:00,  4.98s/it, lr=8.75e-5, optuna_metric=0.0906, step_loss=0.0457]Model weights saved in optuna-sd-model_bs8_lr0.0001_schedlinear_acc8_12/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:01<00:07,  1.17s/it]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:01<00:03,  1.64it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:04<00:02,  1.03s/it]\u001b[A{'final_sigmas_type', 'use_exponential_sigmas', 'sigma_max', 'sigma_min', 'use_beta_sigmas', 'timestep_type', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:04<00:00,  1.41it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [43:41<00:00,  5.17s/it, lr=8.75e-5, optuna_metric=0.0906, step_loss=0.0457]\n",
      "[I 2025-05-18 00:56:26,135] Trial 12 finished with value: 0.09062710026252509 and parameters: {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 16}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 12] optuna_metric: 0.09062710026252509\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': None, 'rank': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/18/2025 00:56:33 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'clip_sample_range', 'thresholding', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "{'use_post_quant_conv', 'latents_mean', 'latents_std', 'shift_factor', 'mid_block_add_attention', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'attention_type', 'reverse_transformer_layers_per_block', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/18/2025 00:56:42 - INFO - __main__ - ***** Running training *****\n",
      "05/18/2025 00:56:42 - INFO - __main__ -   Num examples = 10805\n",
      "05/18/2025 00:56:42 - INFO - __main__ -   Num Epochs = 3\n",
      "05/18/2025 00:56:42 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
      "05/18/2025 00:56:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/18/2025 00:56:42 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/18/2025 00:56:42 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [43:50<00:00,  4.96s/it, lr=8.75e-5, optuna_metric=0.0908, step_loss=0.0484]Model weights saved in optuna-sd-model_bs8_lr0.0001_schedlinear_acc8_13/pytorch_lora_weights.safetensors\n",
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A{'sigma_min', 'rescale_betas_zero_snr', 'final_sigmas_type', 'sigma_max', 'use_exponential_sigmas', 'use_beta_sigmas', 'timestep_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'attention_type', 'reverse_transformer_layers_per_block', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:03<00:04,  1.22s/it]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:03<00:01,  1.46it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\u001b[A\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [43:56<00:00,  5.20s/it, lr=8.75e-5, optuna_metric=0.0908, step_loss=0.0484]\n",
      "[I 2025-05-18 01:40:41,029] Trial 13 finished with value: 0.09077367657249222 and parameters: {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': None, 'rank': 4}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 13] optuna_metric: 0.09077367657249222\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 1e-05, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': None, 'rank': 16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/18/2025 01:40:48 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'clip_sample_range', 'variance_type', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'use_quant_conv', 'shift_factor', 'use_post_quant_conv', 'latents_mean', 'latents_std', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'dropout', 'reverse_transformer_layers_per_block', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/18/2025 01:40:56 - INFO - __main__ - ***** Running training *****\n",
      "05/18/2025 01:40:56 - INFO - __main__ -   Num examples = 10805\n",
      "05/18/2025 01:40:56 - INFO - __main__ -   Num Epochs = 3\n",
      "05/18/2025 01:40:56 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
      "05/18/2025 01:40:56 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/18/2025 01:40:56 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/18/2025 01:40:56 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [43:29<00:00,  4.99s/it, lr=8.75e-6, optuna_metric=0.0925, step_loss=0.0467]Model weights saved in optuna-sd-model_bs8_lr1e-05_schedlinear_acc8_14/pytorch_lora_weights.safetensors\n",
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'dropout', 'reverse_transformer_layers_per_block', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:03<00:09,  1.85s/it]\u001b[A{'use_beta_sigmas', 'use_exponential_sigmas', 'rescale_betas_zero_snr', 'final_sigmas_type', 'sigma_max', 'timestep_type', 'sigma_min'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  57%|█████▋    | 4/7 [00:04<00:03,  1.10s/it]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:05<00:01,  1.20it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [43:36<00:00,  5.16s/it, lr=8.75e-6, optuna_metric=0.0925, step_loss=0.0467]\n",
      "[I 2025-05-18 02:24:35,308] Trial 14 finished with value: 0.09247206643088098 and parameters: {'learning_rate': 1e-05, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': None, 'rank': 16}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 14] optuna_metric: 0.09247206643088098\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 1e-06, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/18/2025 02:24:42 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'clip_sample_range', 'thresholding', 'dynamic_thresholding_ratio', 'variance_type', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "{'shift_factor', 'use_post_quant_conv', 'mid_block_add_attention', 'latents_std', 'use_quant_conv', 'latents_mean'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'attention_type', 'reverse_transformer_layers_per_block', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/18/2025 02:24:50 - INFO - __main__ - ***** Running training *****\n",
      "05/18/2025 02:24:50 - INFO - __main__ -   Num examples = 10805\n",
      "05/18/2025 02:24:50 - INFO - __main__ -   Num Epochs = 3\n",
      "05/18/2025 02:24:50 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
      "05/18/2025 02:24:50 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/18/2025 02:24:50 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/18/2025 02:24:50 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps: 100%|██████████| 507/507 [43:16<00:00,  4.92s/it, lr=8.75e-7, optuna_metric=0.0951, step_loss=0.0476]Model weights saved in optuna-sd-model_bs8_lr1e-06_schedlinear_acc8_15/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A{'use_exponential_sigmas', 'use_beta_sigmas', 'final_sigmas_type', 'timestep_type', 'rescale_betas_zero_snr', 'sigma_min', 'sigma_max'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'attention_type', 'reverse_transformer_layers_per_block', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:03<00:09,  1.83s/it]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:03<00:04,  1.16s/it]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  57%|█████▋    | 4/7 [00:05<00:03,  1.15s/it]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 507/507 [43:23<00:00,  5.13s/it, lr=8.75e-7, optuna_metric=0.0951, step_loss=0.0476]\n",
      "[I 2025-05-18 03:08:16,188] Trial 15 finished with value: 0.09513287907777955 and parameters: {'learning_rate': 1e-06, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 8}. Best is trial 3 with value: 0.09035178066369554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 15] optuna_metric: 0.09513287907777955\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 0.0001, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/18/2025 03:08:23 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'variance_type', 'clip_sample_range', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'use_quant_conv', 'latents_std', 'use_post_quant_conv', 'shift_factor', 'latents_mean', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'dropout', 'attention_type', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/18/2025 03:08:31 - INFO - __main__ - ***** Running training *****\n",
      "05/18/2025 03:08:31 - INFO - __main__ -   Num examples = 10805\n",
      "05/18/2025 03:08:31 - INFO - __main__ -   Num Epochs = 3\n",
      "05/18/2025 03:08:31 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/18/2025 03:08:31 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "05/18/2025 03:08:31 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/18/2025 03:08:31 - INFO - __main__ -   Total optimization steps = 255\n",
      "Steps: 100%|██████████| 255/255 [39:52<00:00,  7.88s/it, lr=8.75e-5, optuna_metric=0.0904, step_loss=0.13]  Model weights saved in optuna-sd-model_bs16_lr0.0001_schedlinear_acc8_16/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'dropout', 'attention_type', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  14%|█▍        | 1/7 [00:03<00:22,  3.69s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:04<00:05,  1.39s/it]\u001b[A{'use_exponential_sigmas', 'use_beta_sigmas', 'rescale_betas_zero_snr', 'timestep_type', 'sigma_min', 'sigma_max', 'final_sigmas_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:05<00:01,  1.36it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 255/255 [39:59<00:00,  9.41s/it, lr=8.75e-5, optuna_metric=0.0904, step_loss=0.13]\n",
      "[I 2025-05-18 03:48:32,697] Trial 16 finished with value: 0.09035168176706065 and parameters: {'learning_rate': 0.0001, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}. Best is trial 16 with value: 0.09035168176706065.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 16] optuna_metric: 0.09035168176706065\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 0.0001, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/18/2025 03:48:39 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'rescale_betas_zero_snr', 'clip_sample_range', 'variance_type', 'dynamic_thresholding_ratio', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'latents_std', 'shift_factor', 'latents_mean', 'use_quant_conv', 'mid_block_add_attention', 'use_post_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/18/2025 03:48:48 - INFO - __main__ - ***** Running training *****\n",
      "05/18/2025 03:48:48 - INFO - __main__ -   Num examples = 10805\n",
      "05/18/2025 03:48:48 - INFO - __main__ -   Num Epochs = 3\n",
      "05/18/2025 03:48:48 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/18/2025 03:48:48 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "05/18/2025 03:48:48 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/18/2025 03:48:48 - INFO - __main__ -   Total optimization steps = 255\n",
      "Steps: 100%|██████████| 255/255 [39:48<00:00,  7.86s/it, lr=8.75e-5, optuna_metric=0.0904, step_loss=0.13]  Model weights saved in optuna-sd-model_bs16_lr0.0001_schedlinear_acc8_17/pytorch_lora_weights.safetensors\n",
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00,  8.91it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "{'rescale_betas_zero_snr', 'sigma_max', 'sigma_min', 'use_beta_sigmas', 'use_exponential_sigmas', 'final_sigmas_type', 'timestep_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:01<00:00,  3.26it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.37it/s]\u001b[A\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 255/255 [39:54<00:00,  9.39s/it, lr=8.75e-5, optuna_metric=0.0904, step_loss=0.13]\n",
      "[I 2025-05-18 04:28:45,070] Trial 17 finished with value: 0.09035166661305993 and parameters: {'learning_rate': 0.0001, 'train_batch_size': 16, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}. Best is trial 17 with value: 0.09035166661305993.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 17] optuna_metric: 0.09035166661305993\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 0.0001, 'train_batch_size': 16, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/18/2025 04:28:52 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'rescale_betas_zero_snr', 'clip_sample_range', 'variance_type', 'thresholding', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "{'use_quant_conv', 'shift_factor', 'use_post_quant_conv', 'latents_mean', 'mid_block_add_attention', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/18/2025 04:29:00 - INFO - __main__ - ***** Running training *****\n",
      "05/18/2025 04:29:00 - INFO - __main__ -   Num examples = 10805\n",
      "05/18/2025 04:29:00 - INFO - __main__ -   Num Epochs = 3\n",
      "05/18/2025 04:29:00 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/18/2025 04:29:00 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "05/18/2025 04:29:00 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/18/2025 04:29:00 - INFO - __main__ -   Total optimization steps = 255\n",
      "Steps: 100%|██████████| 255/255 [39:48<00:00,  7.87s/it, lr=9.62e-5, optuna_metric=0.0903, step_loss=0.13]  Model weights saved in optuna-sd-model_bs16_lr0.0001_schedcosine_acc8_18/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'feature_extractor'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:01<00:03,  1.64it/s]\u001b[A{'rescale_betas_zero_snr', 'sigma_min', 'use_beta_sigmas', 'use_exponential_sigmas', 'timestep_type', 'final_sigmas_type', 'sigma_max'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:01<00:00,  4.12it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'dropout', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  86%|████████▌ | 6/7 [00:05<00:01,  1.08s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 255/255 [39:55<00:00,  9.39s/it, lr=9.62e-5, optuna_metric=0.0903, step_loss=0.13]\n",
      "[I 2025-05-18 05:08:57,981] Trial 18 finished with value: 0.090338858346513 and parameters: {'learning_rate': 0.0001, 'train_batch_size': 16, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}. Best is trial 18 with value: 0.090338858346513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 18] optuna_metric: 0.090338858346513\n",
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 1e-05, 'train_batch_size': 16, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/18/2025 05:09:05 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'variance_type', 'clip_sample_range', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'shift_factor', 'latents_std', 'latents_mean', 'use_quant_conv', 'use_post_quant_conv', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'attention_type', 'dropout', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/18/2025 05:09:13 - INFO - __main__ - ***** Running training *****\n",
      "05/18/2025 05:09:13 - INFO - __main__ -   Num examples = 10805\n",
      "05/18/2025 05:09:13 - INFO - __main__ -   Num Epochs = 3\n",
      "05/18/2025 05:09:13 - INFO - __main__ -   Instantaneous batch size per device = 16\n",
      "05/18/2025 05:09:13 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "05/18/2025 05:09:13 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/18/2025 05:09:13 - INFO - __main__ -   Total optimization steps = 255\n",
      "Steps: 100%|██████████| 255/255 [39:53<00:00,  7.89s/it, lr=9.62e-6, optuna_metric=0.0925, step_loss=0.132] Model weights saved in optuna-sd-model_bs16_lr1e-05_schedcosine_acc8_19/pytorch_lora_weights.safetensors\n",
      "{'feature_extractor', 'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A{'rescale_betas_zero_snr', 'final_sigmas_type', 'use_exponential_sigmas', 'use_beta_sigmas', 'timestep_type', 'sigma_min', 'sigma_max'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as EulerDiscreteScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  29%|██▊       | 2/7 [00:00<00:00,  8.31it/s]\u001b[ALoaded text_encoder_2 as CLIPTextModelWithProjection from `text_encoder_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  43%|████▎     | 3/7 [00:01<00:02,  1.82it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'attention_type', 'dropout', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-xl-base-1.0/snapshots/462165984030d82259a11f4367a4eed129e94a7b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "\n",
      "Loading pipeline components...:  57%|█████▋    | 4/7 [00:05<00:05,  1.71s/it]\u001b[ALoaded tokenizer_2 as CLIPTokenizer from `tokenizer_2` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-xl-base-1.0.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:05<00:00,  1.36it/s]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "Steps: 100%|██████████| 255/255 [40:00<00:00,  9.41s/it, lr=9.62e-6, optuna_metric=0.0925, step_loss=0.132]\n",
      "[I 2025-05-18 05:49:16,056] Trial 19 finished with value: 0.09253397656902962 and parameters: {'learning_rate': 1e-05, 'train_batch_size': 16, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}. Best is trial 18 with value: 0.090338858346513.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 19] optuna_metric: 0.09253397656902962\n",
      "\n",
      "=============================\n",
      "💡 Best trial:\n",
      "FrozenTrial(number=18, state=1, values=[0.090338858346513], datetime_start=datetime.datetime(2025, 5, 18, 4, 28, 45, 71078), datetime_complete=datetime.datetime(2025, 5, 18, 5, 8, 57, 980954), params={'learning_rate': 0.0001, 'train_batch_size': 16, 'lr_scheduler': 'cosine', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': CategoricalDistribution(choices=(1e-05, 5e-05, 0.0001, 1e-06)), 'train_batch_size': CategoricalDistribution(choices=(8, 16)), 'lr_scheduler': CategoricalDistribution(choices=('cosine', 'linear')), 'gradient_accumulation_steps': CategoricalDistribution(choices=(4, 8)), 'snr_gamma': CategoricalDistribution(choices=(None, 5.0)), 'rank': CategoricalDistribution(choices=(4, 8, 16))}, trial_id=18, value=None)\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import subprocess\n",
    "import joblib\n",
    "import optuna.visualization as vis\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "\n",
    "# ========== OBJECTIVE FUNCTION ==========\n",
    "def objective(trial):\n",
    "    # Hyperparamètres à optimiser\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [1e-5, 5e-5, 1e-4, 1e-6])\n",
    "    batch_size = trial.suggest_categorical(\"train_batch_size\", [8, 16])\n",
    "    lr_scheduler = trial.suggest_categorical(\"lr_scheduler\", [\"cosine\", \"linear\"])\n",
    "    gradient_accumulation = trial.suggest_categorical(\"gradient_accumulation_steps\", [4, 8])\n",
    "    snr_gamma = trial.suggest_categorical('snr_gamma', [None, 5.0])\n",
    "    rank = trial.suggest_categorical('rank', [4, 8, 16])\n",
    "    # Nom unique pour chaque essai\n",
    "    output_dir = f\"optuna-sd-model_bs{batch_size}_lr{learning_rate}_sched{lr_scheduler}_acc{gradient_accumulation}_{trial.number}\"\n",
    "\n",
    "    # Commande d'entraînement\n",
    "    command = [\n",
    "        \"accelerate\", \"launch\", \"train_text_to_image_lora_sdxl.py\",\n",
    "        \"--pretrained_model_name_or_path=stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        \"--pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix\",\n",
    "        \"--train_data_dir=/workspace/PFE/VITON-HD-512/train_images\",\n",
    "        \"--caption_column=text\",\n",
    "        \"--dataloader_num_workers=8\",\n",
    "        \"--resolution=512\",\n",
    "        f\"--train_batch_size={batch_size}\",\n",
    "        #f\"--max_train_steps={4052 if batch_size == 8 else 2026}\",\n",
    "        \"--max_grad_norm=1\",\n",
    "        f\"--gradient_accumulation_steps={gradient_accumulation}\",\n",
    "        \"--num_train_epochs=3\",\n",
    "        \"--checkpointing_steps=2026\",\n",
    "        f\"--learning_rate={learning_rate}\",\n",
    "        f\"--lr_scheduler={lr_scheduler}\",\n",
    "        \"--lr_warmup_steps=0\",\n",
    "        \"--mixed_precision=fp16\",\n",
    "        \"--seed=1337\",\n",
    "        \"--allow_tf32\",\n",
    "        \"--use_8bit_adam\",\n",
    "        \"--enable_xformers_memory_efficient_attention\",\n",
    "        f\"--rank={rank}\",\n",
    "        f\"--output_dir={output_dir}\"\n",
    "        #\"--validation_prompt=A portrait of a pregnant black woman in her thirties, plain background.\",\n",
    "        #\"--num_validation_images=0\"    \n",
    "    ]\n",
    "\n",
    "    if snr_gamma is not None:\n",
    "        f\"--snr_gamma={snr_gamma}\",\n",
    "\n",
    "\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"🔁 Lancement de l'entraînement pour les params : {trial.params}\")\n",
    "        result = subprocess.run(\n",
    "            command, \n",
    "             stdout=subprocess.PIPE,\n",
    "            #stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            check=True)\n",
    "    except Exception as e:\n",
    "        print(\"Erreur lors du nettoyage GPU :\", e)\n",
    "\n",
    "  \n",
    "    if 'total_loss' in result.stdout:\n",
    "        for line in result.stdout.splitlines():\n",
    "            if \"total_loss\" in line:\n",
    "                try:\n",
    "                    loss = float(line.split(\"total_loss\")[1].split()[1])\n",
    "                    print(f\"[Trial {trial.number}] optuna_metric: {loss}\")\n",
    "                    return loss\n",
    "                except Exception as e:\n",
    "                    print(f\"[Trial {trial.number}] Erreur extraction train_loss:\", e)\n",
    "\n",
    "    return float(\"inf\")\n",
    "            \n",
    "# ========== LANCER L’OPTIMISATION ==========\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "study_name = f\"sdxl_optuna_study_{timestamp}\"\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=study_name)\n",
    "study.optimize(objective, n_trials=20)\n",
    "# Sauvegarde du modèle d’étude\n",
    "joblib.dump(study, f\"{study_name}.pkl\")\n",
    "print(\"\\n=============================\")\n",
    "print(\"💡 Best trial:\")\n",
    "print(study.best_trial)\n",
    "print(\"=============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Génération des graphiques Optuna...\n",
      "✅ Plots sauvegardés en fichiers HTML.\n"
     ]
    }
   ],
   "source": [
    "# ========== VISUALISATION ==========\n",
    "\n",
    "import optuna.visualization as vis\n",
    "print(\"📊 Génération des graphiques Optuna...\")\n",
    "vis.plot_optimization_history(study).write_html(\"plot_optimization_history.html\", include_plotlyjs=\"cdn\")\n",
    "vis.plot_param_importances(study).write_html(\"plot_param_importance.html\", include_plotlyjs=\"cdn\")\n",
    "vis.plot_parallel_coordinate(study).write_html(\"plot_parallel_coordinates.html\", include_plotlyjs=\"cdn\")\n",
    "vis.plot_slice(study).write_html(\"plot_slice.html\", include_plotlyjs=\"cdn\")\n",
    "vis.plot_contour(study, params=[\"learning_rate\", \"gradient_accumulation_steps\"]).write_html(\"plot_contour.html\", include_plotlyjs=\"cdn\")\n",
    "print(\"✅ Plots sauvegardés en fichiers HTML.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c153ac5a7624ae397f9fb9b25601ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "No LoRA keys associated to CLIPTextModelWithProjection found with the prefix='text_encoder_2'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModelWithProjection related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76b85a3f6574f9ab92f57de093ab517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_path = \"/workspace/PFE/models/diffusers/examples/text_to_image/optuna-sd-model_bs16_lr0.0001_schedcosine_acc8_18/\"\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.load_lora_weights(model_path)\n",
    "\n",
    "prompt = \"A slim white woman in her twenties, white background. full body.\"\n",
    "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
    "image.save(\"/workspace/PFE/models/img.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE9SeCUxHtNa",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SD-2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122069,
     "status": "ok",
     "timestamp": 1747327656881,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "vjPI5xNNSKe3",
    "outputId": "e638ba44-b473-4df0-bec6-ea9564396661"
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2066781,
     "status": "ok",
     "timestamp": 1747330394467,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "om01k-IRHtNb",
    "outputId": "5ca1d962-4df2-4e50-eb62-d04048b52436"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import subprocess\n",
    "import joblib\n",
    "import optuna.visualization as vis\n",
    "from datetime import datetime\n",
    "import sys\n",
    "# ========== OBJECTIVE FUNCTION ==========\n",
    "def objective(trial):\n",
    "    # Hyperparamètres à optimiser\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [1e-5, 5e-5, 1e-4, 1e-6])\n",
    "    batch_size = trial.suggest_categorical(\"train_batch_size\", [8])\n",
    "    lr_scheduler = trial.suggest_categorical(\"lr_scheduler\", [\"cosine\", \"linear\"])\n",
    "    gradient_accumulation = trial.suggest_categorical(\"gradient_accumulation_steps\", [4, 8])\n",
    "\n",
    "    # Nom unique pour chaque essai\n",
    "    output_dir = f\"optuna-sd-model_bs{batch_size}_lr{learning_rate}_sched{lr_scheduler}_acc{gradient_accumulation}_{trial.number}\"\n",
    "\n",
    "    # Commande d'entraînement\n",
    "    command = [\n",
    "        \"accelerate\", \"launch\", \"train_text_to_image_lora.py\",\n",
    "        \"--pretrained_model_name_or_path=stabilityai/stable-diffusion-2-1\",\n",
    "        \"--train_data_dir=/content/drive/MyDrive/PFE/main/PFE/dataset/images\",\n",
    "        \"--caption_column=prompt\",\n",
    "        \"--dataloader_num_workers=8\",\n",
    "        \"--resolution=512\",\n",
    "        f\"--train_batch_size={batch_size}\",\n",
    "        \"--max_train_steps=10\",\n",
    "        \"--max_grad_norm=1\",\n",
    "        f\"--gradient_accumulation_steps={gradient_accumulation}\",\n",
    "        \"--num_train_epochs=2\",\n",
    "        \"--checkpointing_steps=150\",\n",
    "        f\"--learning_rate={learning_rate}\",\n",
    "        f\"--lr_scheduler={lr_scheduler}\",\n",
    "        \"--lr_warmup_steps=0\",\n",
    "        \"--mixed_precision=fp16\",\n",
    "        \"--seed=1337\",\n",
    "        '--allow_tf32',\n",
    "        '--use_8bit_adam',\n",
    "        f\"--output_dir={output_dir}\",\n",
    "        \"--validation_prompt=A portrait of a slim white woman in her twenties, plain background.\"\n",
    "\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n🚀 [Trial {trial.number}] Commande:\\n{' '.join(command)}\\n\")\n",
    "\n",
    "    # Exécution avec affichage direct\n",
    "    #---------------------------------------\n",
    "\n",
    "    import shlex\n",
    "    process = subprocess.Popen(\n",
    "    command,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    bufsize=1\n",
    "    )\n",
    "\n",
    "    log_lines = []\n",
    "    for line in process.stdout:\n",
    "      print(line, end=\"\")  # Affiche chaque ligne en temps réel\n",
    "      log_lines.append(line)\n",
    "\n",
    "    process.wait()\n",
    "    output_text = \"\".join(log_lines)\n",
    "\n",
    "    #result = subprocess.run(command, capture_output=True, text=True)\n",
    "    #print(result.stderr)\n",
    "    #print('-'*30)\n",
    "    #print(result.stdout)\n",
    "    # Extraction de la perte depuis stdout\n",
    "    if \"train_loss\" in output_text:\n",
    "        for line in output_text.splitlines():\n",
    "            if \"train_loss\" in line:\n",
    "                try:\n",
    "                    loss = float(line.split(\"train_loss\")[1].split()[1])\n",
    "                    print(f\"[Trial {trial.number}] train_loss: {loss}\")\n",
    "                    print(loss)\n",
    "                    return loss\n",
    "                except Exception as e:\n",
    "                    print(f\"[Trial {trial.number}] Erreur extraction train_loss:\", e)\n",
    "\n",
    "    return float(\"inf\")  # Mauvais essai si échec\n",
    "\n",
    "\n",
    "# ========== LANCER L’OPTIMISATION ==========\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "study_name = f\"sd_optuna_study_{timestamp}\"\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=study_name)\n",
    "study.optimize(objective, n_trials=5)\n",
    "# Sauvegarde du modèle d’étude\n",
    "joblib.dump(study, f\"{study_name}.pkl\")\n",
    "print(\"\\n=============================\")\n",
    "print(\"💡 Best trial:\")\n",
    "print(study.best_trial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2331,
     "status": "ok",
     "timestamp": 1747330436650,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "dN7cmqOHHtNd",
    "outputId": "c8b3c660-8e5c-4e68-fcc4-87482043bcb9"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "print(\"📊 Génération des graphiques Optuna...\")\n",
    "\n",
    "# Ne garder que les essais réussis\n",
    "completed_trials = [t for t in study.trials if t.value is not None and math.isfinite(t.value)]\n",
    "print(completed_trials)\n",
    "if len(completed_trials) < 2:\n",
    "    print(\"⚠️ Pas assez de résultats valides pour les visualisations avancées.\")\n",
    "    vis.plot_optimization_history(study).write_html(\"plot_optimization_history.html\", include_plotlyjs=\"cdn\")\n",
    "else:\n",
    "    vis.plot_optimization_history(study).write_html(\"plot_optimization_history.html\", include_plotlyjs=\"cdn\")\n",
    "    vis.plot_param_importances(study).write_html(\"plot_param_importance.html\", include_plotlyjs=\"cdn\")\n",
    "    vis.plot_parallel_coordinate(study).write_html(\"plot_parallel_coordinates.html\", include_plotlyjs=\"cdn\")\n",
    "    vis.plot_slice(study).write_html(\"plot_slice.html\", include_plotlyjs=\"cdn\")\n",
    "    vis.plot_contour(study, params=[\"learning_rate\", \"gradient_accumulation_steps\"]).write_html(\"plot_contour.html\", include_plotlyjs=\"cdn\")\n",
    "\n",
    "print(\"✅ Plots sauvegardés en fichiers HTML.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "425fd89fd61349b4b9523bca5ff28852",
      "bcc895cbeb3148c7b999688dc922f893",
      "e6b2bcd071fe424a876cbb27ddb0db2b",
      "fcba60321ea541688822e997f07a00df",
      "d37e4197b96340ca8fe34fc760cc5996",
      "84afc24e75b84cc39e27debc48997f05",
      "a090e99957c14ef591898845b19b126f",
      "63eda89c45c0480bb049bbe7109a5426",
      "ffa3f436a88d4fa3ae89685d08c7e1f7",
      "b3789dee539c4de295f1be955b95e03e",
      "cf6e7fef821c43d28c6fa6937e0ed9c9",
      "3a6ea9ed47f346388c815d1efce53f10",
      "a659f9bac0f34d8eb7daca01b7f265e7",
      "8ae9b29b65434e928462a54c603431d5",
      "d47423d0ce6140429c787ea878535049",
      "e4d3138f4ae446e9839690bdf9d870a6",
      "c7df59da83c143bd8100109b785fdce7",
      "1b38d39289744953b0c59748f5da7eba",
      "fb222cbb942747139701d7d39fdfdcd6",
      "9167a111738e4e4eb1e1c9e42f36282d",
      "a69ae5f5411e4a0d8771114a0a3817fa",
      "ea91a644a3b74340a374d635e7152611"
     ]
    },
    "executionInfo": {
     "elapsed": 38739,
     "status": "ok",
     "timestamp": 1747330990946,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "LtDhy2Nepm-8",
    "outputId": "327429e7-7509-468e-f331-d856673ef3cc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425fd89fd61349b4b9523bca5ff28852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6ea9ed47f346388c815d1efce53f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/PFE/main/PFE/models/diffusers/examples/text_to_image/optuna-sd-model_bs8_lr1e-05_schedlinear_acc8_0\"\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.load_lora_weights(model_path)\n",
    "\n",
    "prompt = \"A portrait of a pregnant white woman in her twenties, plain background.\"\n",
    "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
    "image.save(\"/content/img.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYqq8M5SCHQo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "e7DL-5T6a7y2"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ab709fc478c47ad9b4b32835c08b7f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b6f75c73b4245e5ac011b79ff88e316": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f98e3ac88de4c0a8562509fdddf2414": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce2c638c0b2c425f85992f4cb3ae7914",
      "placeholder": "​",
      "style": "IPY_MODEL_6e213250555c4fae8e8b4e12e4ac528b",
      "value": " 5/5 [00:22&lt;00:00,  4.99s/it]"
     }
    },
    "18f1b6809eb7462eb8992b0ef123ed54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_ea9a99ce48ca40b5955ba6bf981b8904",
      "style": "IPY_MODEL_60af33f0b5b245ddbfdf1b6048dce0ce",
      "tooltip": ""
     }
    },
    "1b38d39289744953b0c59748f5da7eba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24581c35eccb4fcaa24783a372c1369b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d83a0171e7242eabcd9b48ede179ab8",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5160ab7f0ea4475b88c3f22befad8c9f",
      "value": 5
     }
    },
    "30ec6de5c21f4d57b04352fa516c1894": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a6ea9ed47f346388c815d1efce53f10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a659f9bac0f34d8eb7daca01b7f265e7",
       "IPY_MODEL_8ae9b29b65434e928462a54c603431d5",
       "IPY_MODEL_d47423d0ce6140429c787ea878535049"
      ],
      "layout": "IPY_MODEL_e4d3138f4ae446e9839690bdf9d870a6"
     }
    },
    "3de95f399ca14a6b87596df4f19401fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4801fdbb01e54152baf472fca2d45ca4",
      "placeholder": "​",
      "style": "IPY_MODEL_63748aebd8224662b91987cf59a7d239",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "404b03575c404e0caa036ed4ca71713a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "425fd89fd61349b4b9523bca5ff28852": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bcc895cbeb3148c7b999688dc922f893",
       "IPY_MODEL_e6b2bcd071fe424a876cbb27ddb0db2b",
       "IPY_MODEL_fcba60321ea541688822e997f07a00df"
      ],
      "layout": "IPY_MODEL_d37e4197b96340ca8fe34fc760cc5996"
     }
    },
    "47c33d1b613d4020bd95daf95614605c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e972ff5985dc45e2b55c06a54614eb73",
       "IPY_MODEL_24581c35eccb4fcaa24783a372c1369b",
       "IPY_MODEL_0f98e3ac88de4c0a8562509fdddf2414"
      ],
      "layout": "IPY_MODEL_b4f7c1eb7e714503997750d7491e09df"
     }
    },
    "4801fdbb01e54152baf472fca2d45ca4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "486b60aed1e248a59089d373e09255c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5160ab7f0ea4475b88c3f22befad8c9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5745d5f89277404996948f96880dd048": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5aaf113e6c274350927dc7ffba2fd29a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60af33f0b5b245ddbfdf1b6048dce0ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "63748aebd8224662b91987cf59a7d239": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63eda89c45c0480bb049bbe7109a5426": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ceca2f4addf4d6abe4c7c2e9cec59f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2a12f9e346d4e6294df808c7a590e13",
      "placeholder": "​",
      "style": "IPY_MODEL_30ec6de5c21f4d57b04352fa516c1894",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "6e213250555c4fae8e8b4e12e4ac528b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "728750cf0d6240d7b986a90004dbd31b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83571e554b864995bab0f73ff6bf38e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "84afc24e75b84cc39e27debc48997f05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87ef2ee51ab0490bae86566a5d429e2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ae9b29b65434e928462a54c603431d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb222cbb942747139701d7d39fdfdcd6",
      "max": 30,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9167a111738e4e4eb1e1c9e42f36282d",
      "value": 30
     }
    },
    "8d83a0171e7242eabcd9b48ede179ab8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9167a111738e4e4eb1e1c9e42f36282d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "995e0ef750e9461b9ad8c025aa498b83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5745d5f89277404996948f96880dd048",
      "max": 30,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83571e554b864995bab0f73ff6bf38e7",
      "value": 30
     }
    },
    "a06b890f9f944c77a9f3c885290495d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "a090e99957c14ef591898845b19b126f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a659f9bac0f34d8eb7daca01b7f265e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7df59da83c143bd8100109b785fdce7",
      "placeholder": "​",
      "style": "IPY_MODEL_1b38d39289744953b0c59748f5da7eba",
      "value": "100%"
     }
    },
    "a69ae5f5411e4a0d8771114a0a3817fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab0b32dcd5d64ac79370f8322a9dab28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_87ef2ee51ab0490bae86566a5d429e2a",
      "style": "IPY_MODEL_728750cf0d6240d7b986a90004dbd31b",
      "value": true
     }
    },
    "ab9a6d775efd4181ba57cbc887386b79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e60e1220d27b4bc89c765736c18bbb6a",
      "placeholder": "​",
      "style": "IPY_MODEL_404b03575c404e0caa036ed4ca71713a",
      "value": "Connecting..."
     }
    },
    "b3789dee539c4de295f1be955b95e03e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4f7c1eb7e714503997750d7491e09df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6a01add170f4a81a2fe824c4f1e3e7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcc895cbeb3148c7b999688dc922f893": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84afc24e75b84cc39e27debc48997f05",
      "placeholder": "​",
      "style": "IPY_MODEL_a090e99957c14ef591898845b19b126f",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "bd57424576d2425185e34ab3464f7ee1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_a06b890f9f944c77a9f3c885290495d0"
     }
    },
    "c16c7cdda7d14befade3967c3c5fea1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c901c86260be47ad965ae46a7cddaad0",
       "IPY_MODEL_995e0ef750e9461b9ad8c025aa498b83",
       "IPY_MODEL_fd5a2c5a60cb4c52bce596930902fd18"
      ],
      "layout": "IPY_MODEL_0ab709fc478c47ad9b4b32835c08b7f2"
     }
    },
    "c7df59da83c143bd8100109b785fdce7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c901c86260be47ad965ae46a7cddaad0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_486b60aed1e248a59089d373e09255c1",
      "placeholder": "​",
      "style": "IPY_MODEL_0b6f75c73b4245e5ac011b79ff88e316",
      "value": "100%"
     }
    },
    "ca1e5c8ddbee49b78af11138dc4c89f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_b6a01add170f4a81a2fe824c4f1e3e7a",
      "placeholder": "​",
      "style": "IPY_MODEL_cd56972823b2436aa56496e040746628",
      "value": ""
     }
    },
    "cd56972823b2436aa56496e040746628": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce1566c5373f45eb8ef29d39506fd191": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce2c638c0b2c425f85992f4cb3ae7914": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf6e7fef821c43d28c6fa6937e0ed9c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d37e4197b96340ca8fe34fc760cc5996": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d47423d0ce6140429c787ea878535049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a69ae5f5411e4a0d8771114a0a3817fa",
      "placeholder": "​",
      "style": "IPY_MODEL_ea91a644a3b74340a374d635e7152611",
      "value": " 30/30 [00:09&lt;00:00,  3.10it/s]"
     }
    },
    "e4d3138f4ae446e9839690bdf9d870a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e60e1220d27b4bc89c765736c18bbb6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6b2bcd071fe424a876cbb27ddb0db2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63eda89c45c0480bb049bbe7109a5426",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ffa3f436a88d4fa3ae89685d08c7e1f7",
      "value": 6
     }
    },
    "e972ff5985dc45e2b55c06a54614eb73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f47947a165e3431faf026311014037ca",
      "placeholder": "​",
      "style": "IPY_MODEL_5aaf113e6c274350927dc7ffba2fd29a",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "ea4b1c230fa94020bc9039b5d82f2c40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea91a644a3b74340a374d635e7152611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea9a99ce48ca40b5955ba6bf981b8904": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2a12f9e346d4e6294df808c7a590e13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f47947a165e3431faf026311014037ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb222cbb942747139701d7d39fdfdcd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcba60321ea541688822e997f07a00df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3789dee539c4de295f1be955b95e03e",
      "placeholder": "​",
      "style": "IPY_MODEL_cf6e7fef821c43d28c6fa6937e0ed9c9",
      "value": " 6/6 [00:25&lt;00:00,  6.19s/it]"
     }
    },
    "fd5a2c5a60cb4c52bce596930902fd18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce1566c5373f45eb8ef29d39506fd191",
      "placeholder": "​",
      "style": "IPY_MODEL_ea4b1c230fa94020bc9039b5d82f2c40",
      "value": " 30/30 [00:04&lt;00:00,  6.13it/s]"
     }
    },
    "ffa3f436a88d4fa3ae89685d08c7e1f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
