{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ2EOJkpG3ju"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /venv/main/lib/python3.12/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /venv/main/lib/python3.12/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /venv/main/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /venv/main/lib/python3.12/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /venv/main/lib/python3.12/site-packages (from accelerate) (2.7.0+cu128)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /venv/main/lib/python3.12/site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.12/site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /venv/main/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.12/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3274,
     "status": "ok",
     "timestamp": 1747327682266,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "tmo_VJxyHjl-",
    "outputId": "b4218254-4924-4789-f438-ab3de423bdcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /venv/main/lib/python3.12/site-packages (4.3.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /venv/main/lib/python3.12/site-packages (from optuna) (1.15.2)\n",
      "Requirement already satisfied: colorlog in /venv/main/lib/python3.12/site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (from optuna) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.12/site-packages (from optuna) (25.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /venv/main/lib/python3.12/site-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: tqdm in /venv/main/lib/python3.12/site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in /venv/main/lib/python3.12/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in /venv/main/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in /venv/main/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
      "Requirement already satisfied: greenlet>=1 in /venv/main/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /venv/main/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 188160,
     "status": "ok",
     "timestamp": 1747327870429,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "7CoLT0Bf6Ju1",
    "outputId": "f331ad55-cc34-4fab-b0fb-cc77605ea797"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in /venv/main/lib/python3.12/site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: torchvision in /venv/main/lib/python3.12/site-packages (0.22.0+cu128)\n",
      "Requirement already satisfied: torchaudio in /venv/main/lib/python3.12/site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.12/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /venv/main/lib/python3.12/site-packages (from torch) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /venv/main/lib/python3.12/site-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /venv/main/lib/python3.12/site-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /venv/main/lib/python3.12/site-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /venv/main/lib/python3.12/site-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /venv/main/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /venv/main/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /venv/main/lib/python3.12/site-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /venv/main/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /venv/main/lib/python3.12/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7602,
     "status": "ok",
     "timestamp": 1747327878023,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "jDe3P1gP8E0H",
    "outputId": "96db79c5-08d3-4534-d1ad-8435e6b41ef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xformers in /venv/main/lib/python3.12/site-packages (0.0.30)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (from xformers) (2.1.2)\n",
      "Requirement already satisfied: torch==2.7.0 in /venv/main/lib/python3.12/site-packages (from xformers) (2.7.0+cu128)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (70.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /venv/main/lib/python3.12/site-packages (from torch==2.7.0->xformers) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.7.0->xformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.12/site-packages (from jinja2->torch==2.7.0->xformers) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy4mjw_HG70I"
   },
   "source": [
    "## Drive, HuggingFace, Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2057,
     "status": "ok",
     "timestamp": 1747327900768,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "Yc6K--v-QQpn",
    "outputId": "7da8849f-12d2-4ce3-bc39-d1a7f65d00e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1747327900785,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "M3rPcmMMJiF9",
    "outputId": "94192fca-03a9-4e58-a864-836ea86ea6a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/PFE/models/diffusers\n"
     ]
    }
   ],
   "source": [
    "%cd ./diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107,
     "status": "ok",
     "timestamp": 1747327900969,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "S7ylrNAD8VLJ",
    "outputId": "e79ed4a1-11a1-4dd7-8e58-bf10da762118"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CITATION.cff        MANIFEST.in    _typos.toml  \u001b[0m\u001b[01;34mexamples\u001b[0m/       \u001b[01;34msrc\u001b[0m/\n",
      "CODE_OF_CONDUCT.md  Makefile       \u001b[01;34mbenchmarks\u001b[0m/  pyproject.toml  \u001b[01;34mtests\u001b[0m/\n",
      "CONTRIBUTING.md     PHILOSOPHY.md  \u001b[01;34mdocker\u001b[0m/      \u001b[01;34mscripts\u001b[0m/        \u001b[01;34mutils\u001b[0m/\n",
      "LICENSE             README.md      \u001b[01;34mdocs\u001b[0m/        setup.py\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1747327901879,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "O46KTpyq8cX0",
    "outputId": "82b46f64-f90f-4e87-cce4-ce54a47f166d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/PFE/models/diffusers/examples/text_to_image\n"
     ]
    }
   ],
   "source": [
    "%cd examples/text_to_image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "bd57424576d2425185e34ab3464f7ee1",
      "3de95f399ca14a6b87596df4f19401fa",
      "ca1e5c8ddbee49b78af11138dc4c89f7",
      "ab0b32dcd5d64ac79370f8322a9dab28",
      "18f1b6809eb7462eb8992b0ef123ed54",
      "6ceca2f4addf4d6abe4c7c2e9cec59f7",
      "a06b890f9f944c77a9f3c885290495d0",
      "4801fdbb01e54152baf472fca2d45ca4",
      "63748aebd8224662b91987cf59a7d239",
      "b6a01add170f4a81a2fe824c4f1e3e7a",
      "cd56972823b2436aa56496e040746628",
      "87ef2ee51ab0490bae86566a5d429e2a",
      "728750cf0d6240d7b986a90004dbd31b",
      "ea9a99ce48ca40b5955ba6bf981b8904",
      "60af33f0b5b245ddbfdf1b6048dce0ce",
      "f2a12f9e346d4e6294df808c7a590e13",
      "30ec6de5c21f4d57b04352fa516c1894",
      "ab9a6d775efd4181ba57cbc887386b79",
      "e60e1220d27b4bc89c765736c18bbb6a",
      "404b03575c404e0caa036ed4ca71713a"
     ]
    },
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1747327902627,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "StEFcq4U8uOt",
    "outputId": "3780a322-a66a-4835-9865-166333f81ddd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26744c3a1a2e4467940720615bd71b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1747327914164,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "HOdqUN4O9Wol",
    "outputId": "9e8e28dc-bc14-4db5-df11-f0bbeb0d9954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\n",
      "README_sdxl.md\n",
      "\u001b[0m\u001b[01;34moptuna-sd-model_bs8_lr0.0001_schedcosine_acc4_0\u001b[0m/\n",
      "\u001b[01;34moptuna-sd-model_bs8_lr5e-05_schedlinear_acc4_0\u001b[0m/\n",
      "plot_contour.html\n",
      "plot_optimization_history.html\n",
      "plot_parallel_coordinates.html\n",
      "plot_param_importance.html\n",
      "plot_slice.html\n",
      "requirements.txt\n",
      "requirements_flax.txt\n",
      "requirements_sdxl.txt\n",
      "test_text_to_image.py\n",
      "test_text_to_image_lora.py\n",
      "train_text_to_image.py\n",
      "train_text_to_image_flax.py\n",
      "train_text_to_image_lora.py\n",
      "train_text_to_image_lora_sdxl.py\n",
      "train_text_to_image_sdxl.py\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12837,
     "status": "ok",
     "timestamp": 1747327928302,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "tyRvcTCg82wL",
    "outputId": "9b03110f-5a1c-4bb2-94f1-4d09a1e4a862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /root/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    }
   ],
   "source": [
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16410,
     "status": "ok",
     "timestamp": 1747327944714,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "OoHtRaF3-4da",
    "outputId": "7024b60b-bdd6-4c22-f1d7-c2c27f0c12bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: diffusers 0.34.0.dev0\n",
      "Uninstalling diffusers-0.34.0.dev0:\n",
      "  Successfully uninstalled diffusers-0.34.0.dev0\n",
      "Collecting git+https://github.com/huggingface/diffusers.git\n",
      "  Cloning https://github.com/huggingface/diffusers.git to /tmp/pip-req-build-e3yyr69j\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/diffusers.git /tmp/pip-req-build-e3yyr69j\n",
      "  Resolved https://github.com/huggingface/diffusers.git to commit 9836f0e000cfd826a7a5099002253ed2becc13e0\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: importlib_metadata in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (8.7.0)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.27.0 in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (0.30.2)\n",
      "Requirement already satisfied: numpy in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (2.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (0.5.3)\n",
      "Requirement already satisfied: Pillow in /venv/main/lib/python3.12/site-packages (from diffusers==0.34.0.dev0) (11.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.27.0->diffusers==0.34.0.dev0) (4.13.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /venv/main/lib/python3.12/site-packages (from importlib_metadata->diffusers==0.34.0.dev0) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests->diffusers==0.34.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.12/site-packages (from requests->diffusers==0.34.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests->diffusers==0.34.0.dev0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.12/site-packages (from requests->diffusers==0.34.0.dev0) (2025.4.26)\n",
      "Building wheels for collected packages: diffusers\n",
      "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for diffusers: filename=diffusers-0.34.0.dev0-py3-none-any.whl size=3687437 sha256=909400beb40b6b25ffd51f1bcc4393a29729e39f119a31a5d4715192fd0189f7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-c_zw_02u/wheels/23/0f/7d/f97813d265ed0e599a78d83afd4e1925740896ca79b46cccfd\n",
      "Successfully built diffusers\n",
      "Installing collected packages: diffusers\n",
      "Successfully installed diffusers-0.34.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Remove current diffusers\n",
    "!pip uninstall -y diffusers\n",
    "\n",
    "# Install latest diffusers from GitHub (source install)\n",
    "!pip install git+https://github.com/huggingface/diffusers.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4319,
     "status": "ok",
     "timestamp": 1747327949037,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "jZpTJqU-Mv7V",
    "outputId": "59f0c323-65f3-496c-d234-561ee45581df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /venv/main/lib/python3.12/site-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /venv/main/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.12/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /venv/main/lib/python3.12/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /venv/main/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /venv/main/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /venv/main/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /venv/main/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /venv/main/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /venv/main/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /venv/main/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /venv/main/lib/python3.12/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /venv/main/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /venv/main/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /venv/main/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /venv/main/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /venv/main/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /venv/main/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suuJD99yinFj"
   },
   "source": [
    "Les meilleurs modeles:\n",
    "\n",
    "- SD-1.5 \\\\\n",
    "- SD-2.1 \\\\\n",
    "- Realistic_Vision_V6.0 \\\\\n",
    "- SDXL\n",
    "- FLUX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rShfSeR2HYQ2"
   },
   "source": [
    "### REALVISION1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1138863,
     "status": "ok",
     "timestamp": 1746434784044,
     "user": {
      "displayName": "racim saal",
      "userId": "12929633502212961087"
     },
     "user_tz": -60
    },
    "id": "6NjOsrbxg34m",
    "outputId": "e96b9bd7-032f-4de0-e48f-33796d5fa24c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "2025-05-05 08:28:18.597767: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746433698.627617   12012 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746433698.637566   12012 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "05/05/2025 08:28:52 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "scheduler_config.json: 100% 548/548 [00:00<00:00, 2.68MB/s]\n",
      "{'rescale_betas_zero_snr', 'timestep_spacing', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "tokenizer_config.json: 100% 737/737 [00:00<00:00, 3.70MB/s]\n",
      "vocab.json: 100% 1.06M/1.06M [00:00<00:00, 4.73MB/s]\n",
      "merges.txt: 100% 525k/525k [00:00<00:00, 3.79MB/s]\n",
      "special_tokens_map.json: 100% 472/472 [00:00<00:00, 2.70MB/s]\n",
      "config.json: 100% 612/612 [00:00<00:00, 3.41MB/s]\n",
      "model.safetensors: 100% 492M/492M [01:14<00:00, 6.64MB/s]\n",
      "config.json: 100% 577/577 [00:00<00:00, 4.87MB/s]\n",
      "diffusion_pytorch_model.safetensors: 100% 335M/335M [00:41<00:00, 8.04MB/s]\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "config.json: 100% 1.55k/1.55k [00:00<00:00, 9.27MB/s]\n",
      "diffusion_pytorch_model.safetensors: 100% 3.44G/3.44G [05:41<00:00, 10.1MB/s]\n",
      "{'reverse_transformer_layers_per_block', 'encoder_hid_dim_type', 'dropout', 'num_attention_heads', 'attention_type', 'addition_time_embed_dim', 'transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Resolving data files: 100% 101/101 [00:00<00:00, 89315.77it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "05/05/2025 08:37:04 - INFO - __main__ - ***** Running training *****\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Num examples = 100\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Num Epochs = 8\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "05/05/2025 08:37:04 - INFO - __main__ -   Total optimization steps = 200\n",
      "Steps:  12% 25/200 [00:41<04:56,  1.69s/it, lr=9.62e-5, step_loss=0.194]\n",
      "model_index.json: 100% 609/609 [00:00<00:00, 3.42MB/s]\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:02<00:08,  2.03s/it]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  60% 3/5 [00:02<00:01,  1.77it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:03<00:00,  1.37it/s]\n",
      "05/05/2025 08:37:50 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  25% 50/200 [01:46<04:01,  1.61s/it, lr=8.54e-5, step_loss=0.287]  {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:02<00:10,  2.69s/it]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:04<00:00,  1.17it/s]\n",
      "05/05/2025 08:38:55 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  38% 75/200 [02:53<03:20,  1.60s/it, lr=6.91e-5, step_loss=0.088] {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:01<00:05,  1.28s/it]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  60% 3/5 [00:01<00:00,  2.69it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:01<00:00,  3.08it/s]\n",
      "05/05/2025 08:39:59 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  50% 100/200 [03:55<02:35,  1.55s/it, lr=5e-5, step_loss=0.00566]  {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:02,  1.65it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:00<00:00,  5.95it/s]\n",
      "05/05/2025 08:41:01 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  62% 125/200 [04:58<01:58,  1.58s/it, lr=3.09e-5, step_loss=0.00452]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:01,  3.26it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:00<00:00,  9.20it/s]\n",
      "05/05/2025 08:42:03 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  75% 150/200 [06:00<01:18,  1.57s/it, lr=1.46e-5, step_loss=0.0425] {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:01,  2.66it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  60% 3/5 [00:00<00:00,  7.35it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:00<00:00,  7.05it/s]\n",
      "05/05/2025 08:43:06 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps:  88% 175/200 [07:02<00:38,  1.56s/it, lr=3.81e-6, step_loss=0.00444]{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:01,  3.52it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:00<00:00,  9.47it/s]\n",
      "05/05/2025 08:44:07 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Steps: 100% 200/200 [08:04<00:00,  1.64s/it, lr=0, step_loss=0.47]       {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:01,  2.35it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:00<00:00,  7.31it/s]\n",
      "05/05/2025 08:45:09 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "Model weights saved in sd-tryon-realistic_vision-lora/pytorch_lora_weights.safetensors\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0% 0/5 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  20% 1/5 [00:00<00:01,  3.05it/s]\u001b[A{'flow_shift', 'timestep_spacing', 'use_karras_sigmas', 'use_beta_sigmas', 'use_flow_sigmas', 'use_exponential_sigmas'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DEISMultistepScheduler from `scheduler` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...:  80% 4/5 [00:00<00:00,  7.28it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'reverse_transformer_layers_per_block', 'encoder_hid_dim_type', 'dropout', 'num_attention_heads', 'attention_type', 'addition_time_embed_dim', 'transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--SG161222--Realistic_Vision_V5.1_noVAE/snapshots/672a55008829cf5e581a6a33159daf03c44a0f0b/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of SG161222/Realistic_Vision_V5.1_noVAE.\n",
      "\n",
      "Loading pipeline components...: 100% 5/5 [00:16<00:00,  3.25s/it]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "05/05/2025 08:45:49 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: a slim white woman in thirties..\n",
      "\n",
      "README.md: 100% 1.19k/1.19k [00:00<00:00, 9.43MB/s]\n",
      "\n",
      "image_0.png:   0% 0.00/312k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "image_1.png:   0% 0.00/342k [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "image_2.png:   0% 0.00/348k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "image_3.png:   0% 0.00/326k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "events.out.tfevents.1746434224.998770d6f100.12012.1:   0% 0.00/2.40k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 8 LFS files:   0% 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "image_0.png:   5% 16.4k/312k [00:00<00:02, 122kB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "image_3.png:   5% 16.4k/326k [00:00<00:02, 130kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "image_2.png:   5% 16.4k/348k [00:00<00:02, 123kB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "image_1.png:   5% 16.4k/342k [00:00<00:02, 119kB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "events.out.tfevents.1746434224.998770d6f100.12012.1: 100% 2.40k/2.40k [00:00<00:00, 9.06kB/s]\n",
      "image_3.png: 100% 326k/326k [00:00<00:00, 1.03MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "events.out.tfevents.1746432588.998770d6f100.7012.0:   0% 0.00/9.95M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "image_0.png: 100% 312k/312k [00:00<00:00, 651kB/s] \n",
      "image_1.png: 100% 342k/342k [00:00<00:00, 624kB/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "events.out.tfevents.1746434224.998770d6f100.12012.0:  89% 10.7M/12.0M [00:00<00:00, 96.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "image_2.png: 100% 348k/348k [00:00<00:00, 594kB/s] \n",
      "\n",
      "pytorch_lora_weights.safetensors:   0% 0.00/3.23M [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 8 LFS files:  12% 1/8 [00:00<00:04,  1.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "events.out.tfevents.1746434224.998770d6f100.12012.0: 100% 12.0M/12.0M [00:00<00:00, 31.3MB/s]\n",
      "events.out.tfevents.1746432588.998770d6f100.7012.0: 100% 9.95M/9.95M [00:00<00:00, 13.1MB/s]\n",
      "pytorch_lora_weights.safetensors: 100% 3.23M/3.23M [00:00<00:00, 4.99MB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 8 LFS files:  75% 6/8 [00:01<00:00,  5.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Upload 8 LFS files: 100% 8/8 [00:01<00:00,  5.81it/s]\n",
      "Steps: 100% 200/200 [09:11<00:00,  2.76s/it, lr=0, step_loss=0.47]\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train_text_to_image_lora.py \\\n",
    "  --pretrained_model_name_or_path=\"SG161222/Realistic_Vision_V5.1_noVAE\" \\\n",
    "  --train_data_dir=\"/content/drive/MyDrive/PFE/mini/mini/images\" --caption_column=\"text\" \\\n",
    "  --dataloader_num_workers=8 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --max_train_steps=200 \\\n",
    "  --max_grad_norm=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --num_train_epochs=2 --checkpointing_steps=500 \\\n",
    "  --learning_rate=1e-04 --lr_scheduler=\"cosine\" --lr_warmup_steps=0 \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --seed=1337 \\\n",
    "  --output_dir=\"sd-tryon-realistic_vision-lora\" \\\n",
    "  --validation_prompt=\"a slim white woman in thirties.\" \\\n",
    "  --push_to_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "47c33d1b613d4020bd95daf95614605c",
      "e972ff5985dc45e2b55c06a54614eb73",
      "24581c35eccb4fcaa24783a372c1369b",
      "0f98e3ac88de4c0a8562509fdddf2414",
      "b4f7c1eb7e714503997750d7491e09df",
      "f47947a165e3431faf026311014037ca",
      "5aaf113e6c274350927dc7ffba2fd29a",
      "8d83a0171e7242eabcd9b48ede179ab8",
      "5160ab7f0ea4475b88c3f22befad8c9f",
      "ce2c638c0b2c425f85992f4cb3ae7914",
      "6e213250555c4fae8e8b4e12e4ac528b",
      "c16c7cdda7d14befade3967c3c5fea1d",
      "c901c86260be47ad965ae46a7cddaad0",
      "995e0ef750e9461b9ad8c025aa498b83",
      "fd5a2c5a60cb4c52bce596930902fd18",
      "0ab709fc478c47ad9b4b32835c08b7f2",
      "486b60aed1e248a59089d373e09255c1",
      "0b6f75c73b4245e5ac011b79ff88e316",
      "5745d5f89277404996948f96880dd048",
      "83571e554b864995bab0f73ff6bf38e7",
      "ce1566c5373f45eb8ef29d39506fd191",
      "ea4b1c230fa94020bc9039b5d82f2c40"
     ]
    },
    "executionInfo": {
     "elapsed": 29233,
     "status": "ok",
     "timestamp": 1746435500176,
     "user": {
      "displayName": "racim saal",
      "userId": "12929633502212961087"
     },
     "user_tz": -60
    },
    "id": "hu8NbclcbW-Z",
    "outputId": "13aa6ad1-1dff-4af8-acef-9d8a6f90b110"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c33d1b613d4020bd95daf95614605c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16c7cdda7d14befade3967c3c5fea1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_path = \"/content/diffusers/examples/text_to_image/sd-tryon-realistic_vision-lora\"\n",
    "pipe = DiffusionPipeline.from_pretrained(\"SG161222/Realistic_Vision_V5.1_noVAE\", torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.load_lora_weights(model_path)\n",
    "\n",
    "prompt = \"A white average woman wearing white clothes in twenties. White background. full body image.\"\n",
    "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
    "image.save(\"/content/img.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ciAxLWjMkF1C"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7DL-5T6a7y2"
   },
   "source": [
    "## SDXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mr7y8LHSG2gi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 14:56:02,332] A new study created in memory with name: sdxl_optuna_study_20250517_145602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔁 Lancement de l'entraînement pour les params : {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8, 'snr_gamma': 5.0, 'rank': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 14:56:09 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'rescale_betas_zero_snr', 'dynamic_thresholding_ratio', 'variance_type', 'clip_sample_range', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'use_quant_conv', 'latents_mean', 'mid_block_add_attention', 'shift_factor', 'latents_std', 'use_post_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'reverse_transformer_layers_per_block', 'attention_type', 'dropout'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "05/17/2025 14:56:18 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 14:56:18 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 14:56:18 - INFO - __main__ -   Num Epochs = 3\n",
      "05/17/2025 14:56:18 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
      "05/17/2025 14:56:18 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "05/17/2025 14:56:18 - INFO - __main__ -   Gradient Accumulation steps = 8\n",
      "05/17/2025 14:56:18 - INFO - __main__ -   Total optimization steps = 507\n",
      "Steps:   4%|▍         | 22/507 [01:57<41:20,  5.11s/it, lr=9.95e-5, optuna_metric=0.0914, step_loss=0.0933]"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import subprocess\n",
    "import joblib\n",
    "import optuna.visualization as vis\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "# ========== OBJECTIVE FUNCTION ==========\n",
    "def objective(trial):\n",
    "    # Hyperparamètres à optimiser\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [1e-5, 5e-5, 1e-4, 1e-6])\n",
    "    batch_size = trial.suggest_categorical(\"train_batch_size\", [8, 16])\n",
    "    lr_scheduler = trial.suggest_categorical(\"lr_scheduler\", [\"cosine\", \"linear\"])\n",
    "    gradient_accumulation = trial.suggest_categorical(\"gradient_accumulation_steps\", [4, 8])\n",
    "    snr_gamma = trial.suggest_categorical('snr_gamma', [None, 5.0])\n",
    "    rank = trial.suggest_categorical('rank', [4, 8, 16])\n",
    "    # Nom unique pour chaque essai\n",
    "    output_dir = f\"optuna-sd-model_bs{batch_size}_lr{learning_rate}_sched{lr_scheduler}_acc{gradient_accumulation}_{trial.number}\"\n",
    "\n",
    "    # Commande d'entraînement\n",
    "    command = [\n",
    "        \"accelerate\", \"launch\", \"train_text_to_image_lora_sdxl.py\",\n",
    "        \"--pretrained_model_name_or_path=stabilityai/stable-diffusion-xl-base-1.0\",\n",
    "        \"--pretrained_vae_model_name_or_path=madebyollin/sdxl-vae-fp16-fix\",\n",
    "        \"--train_data_dir=/workspace/PFE/VITON-HD-512/train_images\",\n",
    "        \"--caption_column=text\",\n",
    "        \"--dataloader_num_workers=8\",\n",
    "        \"--resolution=512\",\n",
    "        f\"--train_batch_size={batch_size}\",\n",
    "        #f\"--max_train_steps={4052 if batch_size == 8 else 2026}\",\n",
    "        \"--max_grad_norm=1\",\n",
    "        f\"--gradient_accumulation_steps={gradient_accumulation}\",\n",
    "        \"--num_train_epochs=3\",\n",
    "        \"--checkpointing_steps=2026\",\n",
    "        f\"--learning_rate={learning_rate}\",\n",
    "        f\"--lr_scheduler={lr_scheduler}\",\n",
    "        \"--lr_warmup_steps=0\",\n",
    "        \"--mixed_precision=fp16\",\n",
    "        \"--seed=1337\",\n",
    "        \"--allow_tf32\",\n",
    "        \"--use_8bit_adam\",\n",
    "        \"--enable_xformers_memory_efficient_attention\",\n",
    "        f\"--rank={rank}\",\n",
    "        f\"--output_dir={output_dir}\"\n",
    "        #\"--validation_prompt=A portrait of a pregnant black woman in her thirties, plain background.\",\n",
    "        #\"--num_validation_images=0\"    \n",
    "    ]\n",
    "\n",
    "    if snr_gamma is not None:\n",
    "        f\"--snr_gamma={snr_gamma}\",\n",
    "    # result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    import sys\n",
    "    import subprocess\n",
    "\n",
    "#    process = subprocess.Popen(\n",
    "#        command,\n",
    "#        stdout=subprocess.PIPE,\n",
    "#        stderr=subprocess.STDOUT,\n",
    "#        text=True,\n",
    "#        bufsize=1,\n",
    "#        universal_newlines=True\n",
    " #   )\n",
    "\n",
    "  #  log_lines = []\n",
    "   # for line in process.stdout:\n",
    "    #    print(line, end='')  # Affiche en temps réel\n",
    "     #   log_lines.append(line)\n",
    "\n",
    "    #process.wait()  # Attend la fin du process\n",
    "    #result_output = ''.join(log_lines)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"🔁 Lancement de l'entraînement pour les params : {trial.params}\")\n",
    "        result = subprocess.run(\n",
    "            command, \n",
    "             stdout=subprocess.PIPE,\n",
    "            #stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            check=True)\n",
    "    except Exception as e:\n",
    "        print(\"Erreur lors du nettoyage GPU :\", e)\n",
    "\n",
    "  \n",
    "    if 'total_loss' in result.stdout:\n",
    "        for line in result.stdout.splitlines():\n",
    "            if \"total_loss\" in line:\n",
    "                try:\n",
    "                    loss = float(line.split(\"total_loss\")[1].split()[1])\n",
    "                    print(f\"[Trial {trial.number}] optuna_metric: {loss}\")\n",
    "                    return loss\n",
    "                except Exception as e:\n",
    "                    print(f\"[Trial {trial.number}] Erreur extraction train_loss:\", e)\n",
    "\n",
    "    return float(\"inf\")\n",
    "            \n",
    "    \n",
    "\n",
    "# ========== LANCER L’OPTIMISATION ==========\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "study_name = f\"sdxl_optuna_study_{timestamp}\"\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=study_name)\n",
    "study.optimize(objective, n_trials=20)\n",
    "# Sauvegarde du modèle d’étude\n",
    "joblib.dump(study, f\"{study_name}.pkl\")\n",
    "print(\"\\n=============================\")\n",
    "print(\"💡 Best trial:\")\n",
    "print(study.best_trial)\n",
    "print(\"=============================\")\n",
    "# ========== VISUALISATION ==========\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "print(\"📊 Génération des graphiques Optuna...\")\n",
    "vis.plot_optimization_history(study).write_html(\"plot_optimization_history.html\", include_plotlyjs=\"cdn\")\n",
    "vis.plot_param_importances(study).write_html(\"plot_param_importance.html\", include_plotlyjs=\"cdn\")\n",
    "vis.plot_parallel_coordinate(study).write_html(\"plot_parallel_coordinates.html\", include_plotlyjs=\"cdn\")\n",
    "vis.plot_slice(study).write_html(\"plot_slice.html\", include_plotlyjs=\"cdn\")\n",
    "vis.plot_contour(study, params=[\"learning_rate\", \"gradient_accumulation_steps\"]).write_html(\"plot_contour.html\", include_plotlyjs=\"cdn\")\n",
    "print(\"✅ Plots sauvegardés en fichiers HTML.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 800814,
     "status": "ok",
     "timestamp": 1746461389690,
     "user": {
      "displayName": "Délégation_s2i promo2023",
      "userId": "02921373031657368560"
     },
     "user_tz": -60
    },
    "id": "XuzZsTTV-xtS",
    "outputId": "a7655442-00e9-4907-f2bc-b724cf7c12a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py:498: UserWarning: `log_with=tensorboard` was passed but no supported trackers are currently installed.\n",
      "  warnings.warn(f\"`log_with={log_with}` was passed but no supported trackers are currently installed.\")\n",
      "05/17/2025 11:19:07 - INFO - __main__ - Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'thresholding', 'clip_sample_range', 'rescale_betas_zero_snr', 'variance_type', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "{'use_quant_conv', 'use_post_quant_conv', 'shift_factor', 'mid_block_add_attention', 'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at madebyollin/sdxl-vae-fp16-fix.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'attention_type', 'dropout', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-xl-base-1.0.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Resolving data files: 100%|████████████| 10806/10806 [00:00<00:00, 22405.98it/s]\n",
      "Downloading data: 100%|█████████████| 10806/10806 [00:00<00:00, 23520.56files/s]\n",
      "Generating train split: 10805 examples [00:00, 45088.65 examples/s]\n",
      "05/17/2025 11:19:17 - INFO - __main__ - ***** Running training *****\n",
      "05/17/2025 11:19:17 - INFO - __main__ -   Num examples = 10805\n",
      "05/17/2025 11:19:17 - INFO - __main__ -   Num Epochs = 1\n",
      "05/17/2025 11:19:17 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "05/17/2025 11:19:17 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "05/17/2025 11:19:17 - INFO - __main__ -   Gradient Accumulation steps = 4\n",
      "05/17/2025 11:19:17 - INFO - __main__ -   Total optimization steps = 50\n",
      "Steps:   2%|▏        | 1/50 [00:02<02:21,  2.89s/it, lr=0.0001, step_loss=0.214]optuna_metric: 0.064772\n",
      "Steps:   2%|▏        | 1/50 [00:04<02:21,  2.89s/it, lr=0.0001, step_loss=0.186]^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/PFE/models/diffusers/examples/text_to_image/train_text_to_image_lora_sdxl.py\", line 1343, in <module>\n",
      "    main(args)\n",
      "  File \"/workspace/PFE/models/diffusers/examples/text_to_image/train_text_to_image_lora_sdxl.py\", line 1204, in main\n",
      "    accelerator.backward(loss)\n",
      "  File \"/venv/main/lib/python3.12/site-packages/accelerate/accelerator.py\", line 2469, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/venv/main/lib/python3.12/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch train_text_to_image_lora_sdxl.py \\\n",
    "  --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-xl-base-1.0\" \\\n",
    "  --pretrained_vae_model_name_or_path=\"madebyollin/sdxl-vae-fp16-fix\" \\\n",
    "  --train_data_dir=\"/workspace/PFE/VITON-HD-512/train_images\" --caption_column=\"text\" \\\n",
    "  --dataloader_num_workers=8 \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --max_train_steps=50 \\\n",
    "  --max_grad_norm=1 \\\n",
    "  --gradient_accumulation_steps=4 \\\n",
    "  --num_train_epochs=2 --checkpointing_steps=500 \\\n",
    "  --learning_rate=1e-04 --lr_scheduler=\"cosine\" --lr_warmup_steps=0 \\\n",
    "  --mixed_precision=\"fp16\" \\\n",
    "  --seed=1337 \\\n",
    "  --output_dir=\"sd-tryon-model-lora-sdxl\" \\\n",
    "  --validation_prompt=\"a slim black woman in forties\" \\\n",
    "  --push_to_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE9SeCUxHtNa"
   },
   "source": [
    "## SD-2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122069,
     "status": "ok",
     "timestamp": 1747327656881,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "vjPI5xNNSKe3",
    "outputId": "e638ba44-b473-4df0-bec6-ea9564396661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2066781,
     "status": "ok",
     "timestamp": 1747330394467,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "om01k-IRHtNb",
    "outputId": "5ca1d962-4df2-4e50-eb62-d04048b52436"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 16:58:47,610] A new study created in memory with name: sd_optuna_study_20250515_165847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 [Trial 0] Commande:\n",
      "accelerate launch train_text_to_image_lora.py --pretrained_model_name_or_path=stabilityai/stable-diffusion-2-1 --train_data_dir=/content/drive/MyDrive/PFE/main/PFE/dataset/images --caption_column=prompt --dataloader_num_workers=8 --resolution=512 --train_batch_size=8 --max_train_steps=10 --max_grad_norm=1 --gradient_accumulation_steps=8 --num_train_epochs=2 --checkpointing_steps=150 --learning_rate=5e-05 --lr_scheduler=linear --lr_warmup_steps=0 --mixed_precision=fp16 --seed=1337 --allow_tf32 --use_8bit_adam --output_dir=optuna-sd-model_bs8_lr5e-05_schedlinear_acc8_0 --validation_prompt=A portrait of a slim white woman in her twenties, plain background.\n",
      "\n",
      "2025-05-15 16:59:05.607810: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747328345.637116   11496 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747328345.646625   11496 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-15 16:59:05.694692: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:__main__:Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'timestep_spacing', 'variance_type', 'thresholding', 'clip_sample_range', 'sample_max_value', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "{'use_quant_conv', 'force_upcast', 'shift_factor', 'latents_mean', 'scaling_factor', 'use_post_quant_conv', 'mid_block_add_attention', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'resnet_time_scale_shift', 'conv_out_kernel', 'class_embeddings_concat', 'resnet_out_scale_factor', 'time_embedding_act_fn', 'cross_attention_norm', 'mid_block_type', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'projection_class_embeddings_input_dim', 'dropout', 'class_embed_type', 'transformer_layers_per_block', 'addition_embed_type_num_heads', 'addition_embed_type', 'mid_block_only_cross_attention', 'time_embedding_dim', 'encoder_hid_dim_type', 'addition_time_embed_dim', 'attention_type', 'resnet_skip_time_act', 'time_cond_proj_dim', 'encoder_hid_dim', 'conv_in_kernel', 'num_attention_heads', 'timestep_post_act'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "INFO:__main__:***** Running training *****\n",
      "INFO:__main__:  Num examples = 10805\n",
      "INFO:__main__:  Num Epochs = 1\n",
      "INFO:__main__:  Instantaneous batch size per device = 8\n",
      "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "INFO:__main__:  Gradient Accumulation steps = 8\n",
      "INFO:__main__:  Total optimization steps = 10\n",
      "\n",
      "Steps:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Steps:   0%|          | 0/10 [00:09<?, ?it/s, lr=5e-5, step_loss=0.252]\n",
      "Steps:   0%|          | 0/10 [00:11<?, ?it/s, lr=5e-5, step_loss=0.316]\n",
      "Steps:   0%|          | 0/10 [00:13<?, ?it/s, lr=5e-5, step_loss=0.268]\n",
      "Steps:   0%|          | 0/10 [00:16<?, ?it/s, lr=5e-5, step_loss=0.269]\n",
      "Steps:   0%|          | 0/10 [00:18<?, ?it/s, lr=5e-5, step_loss=0.281]\n",
      "Steps:   0%|          | 0/10 [00:20<?, ?it/s, lr=5e-5, step_loss=0.214]\n",
      "Steps:   0%|          | 0/10 [00:22<?, ?it/s, lr=5e-5, step_loss=0.268]\n",
      "Steps:  10%|█         | 1/10 [00:24<03:44, 24.99s/it, lr=5e-5, step_loss=0.268]\n",
      "Steps:  10%|█         | 1/10 [00:32<03:44, 24.99s/it, lr=4.5e-5, step_loss=0.271]\n",
      "Steps:  10%|█         | 1/10 [00:34<03:44, 24.99s/it, lr=4.5e-5, step_loss=0.234]\n",
      "Steps:  10%|█         | 1/10 [00:36<03:44, 24.99s/it, lr=4.5e-5, step_loss=0.274]\n",
      "Steps:  10%|█         | 1/10 [00:38<03:44, 24.99s/it, lr=4.5e-5, step_loss=0.194]\n",
      "Steps:  10%|█         | 1/10 [00:40<03:44, 24.99s/it, lr=4.5e-5, step_loss=0.225]\n",
      "Steps:  10%|█         | 1/10 [00:43<03:44, 24.99s/it, lr=4.5e-5, step_loss=0.279]\n",
      "Steps:  10%|█         | 1/10 [00:45<03:44, 24.99s/it, lr=4.5e-5, step_loss=0.276]\n",
      "Steps:  10%|█         | 1/10 [00:47<03:44, 24.99s/it, lr=4.5e-5, step_loss=0.309]\n",
      "Steps:  20%|██        | 2/10 [02:22<10:34, 79.33s/it, lr=4.5e-5, step_loss=0.309]\n",
      "Steps:  20%|██        | 2/10 [02:22<10:34, 79.33s/it, lr=4e-5, step_loss=0.252]  \n",
      "Steps:  20%|██        | 2/10 [02:24<10:34, 79.33s/it, lr=4e-5, step_loss=0.243]\n",
      "Steps:  20%|██        | 2/10 [02:26<10:34, 79.33s/it, lr=4e-5, step_loss=0.266]\n",
      "Steps:  20%|██        | 2/10 [02:28<10:34, 79.33s/it, lr=4e-5, step_loss=0.223]\n",
      "Steps:  20%|██        | 2/10 [02:30<10:34, 79.33s/it, lr=4e-5, step_loss=0.228]\n",
      "Steps:  20%|██        | 2/10 [02:32<10:34, 79.33s/it, lr=4e-5, step_loss=0.306]\n",
      "Steps:  20%|██        | 2/10 [02:34<10:34, 79.33s/it, lr=4e-5, step_loss=0.27] \n",
      "Steps:  20%|██        | 2/10 [02:37<10:34, 79.33s/it, lr=4e-5, step_loss=0.262]\n",
      "Steps:  30%|███       | 3/10 [02:39<05:55, 50.83s/it, lr=4e-5, step_loss=0.262]\n",
      "Steps:  30%|███       | 3/10 [02:39<05:55, 50.83s/it, lr=3.5e-5, step_loss=0.231]\n",
      "Steps:  30%|███       | 3/10 [02:41<05:55, 50.83s/it, lr=3.5e-5, step_loss=0.294]\n",
      "Steps:  30%|███       | 3/10 [02:43<05:55, 50.83s/it, lr=3.5e-5, step_loss=0.224]\n",
      "Steps:  30%|███       | 3/10 [02:45<05:55, 50.83s/it, lr=3.5e-5, step_loss=0.281]\n",
      "Steps:  30%|███       | 3/10 [02:47<05:55, 50.83s/it, lr=3.5e-5, step_loss=0.246]\n",
      "Steps:  30%|███       | 3/10 [02:50<05:55, 50.83s/it, lr=3.5e-5, step_loss=0.233]\n",
      "Steps:  30%|███       | 3/10 [02:52<05:55, 50.83s/it, lr=3.5e-5, step_loss=0.308]\n",
      "Steps:  30%|███       | 3/10 [02:54<05:55, 50.83s/it, lr=3.5e-5, step_loss=0.261]\n",
      "Steps:  40%|████      | 4/10 [02:56<03:45, 37.63s/it, lr=3.5e-5, step_loss=0.261]\n",
      "Steps:  40%|████      | 4/10 [02:56<03:45, 37.63s/it, lr=3e-5, step_loss=0.221]  \n",
      "Steps:  40%|████      | 4/10 [02:58<03:45, 37.63s/it, lr=3e-5, step_loss=0.292]\n",
      "Steps:  40%|████      | 4/10 [03:01<03:45, 37.63s/it, lr=3e-5, step_loss=0.271]\n",
      "Steps:  40%|████      | 4/10 [03:03<03:45, 37.63s/it, lr=3e-5, step_loss=0.227]\n",
      "Steps:  40%|████      | 4/10 [03:05<03:45, 37.63s/it, lr=3e-5, step_loss=0.267]\n",
      "Steps:  40%|████      | 4/10 [03:07<03:45, 37.63s/it, lr=3e-5, step_loss=0.23] \n",
      "Steps:  40%|████      | 4/10 [03:09<03:45, 37.63s/it, lr=3e-5, step_loss=0.216]\n",
      "Steps:  40%|████      | 4/10 [03:11<03:45, 37.63s/it, lr=3e-5, step_loss=0.259]\n",
      "Steps:  50%|█████     | 5/10 [03:13<02:31, 30.24s/it, lr=3e-5, step_loss=0.259]\n",
      "Steps:  50%|█████     | 5/10 [03:13<02:31, 30.24s/it, lr=2.5e-5, step_loss=0.264]\n",
      "Steps:  50%|█████     | 5/10 [03:15<02:31, 30.24s/it, lr=2.5e-5, step_loss=0.243]\n",
      "Steps:  50%|█████     | 5/10 [03:18<02:31, 30.24s/it, lr=2.5e-5, step_loss=0.232]\n",
      "Steps:  50%|█████     | 5/10 [03:20<02:31, 30.24s/it, lr=2.5e-5, step_loss=0.219]\n",
      "Steps:  50%|█████     | 5/10 [03:22<02:31, 30.24s/it, lr=2.5e-5, step_loss=0.289]\n",
      "Steps:  50%|█████     | 5/10 [03:24<02:31, 30.24s/it, lr=2.5e-5, step_loss=0.293]\n",
      "Steps:  50%|█████     | 5/10 [03:26<02:31, 30.24s/it, lr=2.5e-5, step_loss=0.246]\n",
      "Steps:  50%|█████     | 5/10 [03:28<02:31, 30.24s/it, lr=2.5e-5, step_loss=0.246]\n",
      "Steps:  60%|██████    | 6/10 [03:30<01:42, 25.64s/it, lr=2.5e-5, step_loss=0.246]\n",
      "Steps:  60%|██████    | 6/10 [03:30<01:42, 25.64s/it, lr=2e-5, step_loss=0.246]  \n",
      "Steps:  60%|██████    | 6/10 [03:32<01:42, 25.64s/it, lr=2e-5, step_loss=0.26] \n",
      "Steps:  60%|██████    | 6/10 [03:34<01:42, 25.64s/it, lr=2e-5, step_loss=0.269]\n",
      "Steps:  60%|██████    | 6/10 [03:36<01:42, 25.64s/it, lr=2e-5, step_loss=0.275]\n",
      "Steps:  60%|██████    | 6/10 [03:38<01:42, 25.64s/it, lr=2e-5, step_loss=0.214]\n",
      "Steps:  60%|██████    | 6/10 [03:40<01:42, 25.64s/it, lr=2e-5, step_loss=0.295]\n",
      "Steps:  60%|██████    | 6/10 [03:42<01:42, 25.64s/it, lr=2e-5, step_loss=0.287]\n",
      "Steps:  60%|██████    | 6/10 [03:44<01:42, 25.64s/it, lr=2e-5, step_loss=0.294]\n",
      "Steps:  70%|███████   | 7/10 [03:47<01:08, 22.67s/it, lr=2e-5, step_loss=0.294]\n",
      "Steps:  70%|███████   | 7/10 [03:47<01:08, 22.67s/it, lr=1.5e-5, step_loss=0.243]\n",
      "Steps:  70%|███████   | 7/10 [03:49<01:08, 22.67s/it, lr=1.5e-5, step_loss=0.275]\n",
      "Steps:  70%|███████   | 7/10 [03:51<01:08, 22.67s/it, lr=1.5e-5, step_loss=0.314]\n",
      "Steps:  70%|███████   | 7/10 [03:53<01:08, 22.67s/it, lr=1.5e-5, step_loss=0.24] \n",
      "Steps:  70%|███████   | 7/10 [03:55<01:08, 22.67s/it, lr=1.5e-5, step_loss=0.228]\n",
      "Steps:  70%|███████   | 7/10 [03:57<01:08, 22.67s/it, lr=1.5e-5, step_loss=0.253]\n",
      "Steps:  70%|███████   | 7/10 [03:59<01:08, 22.67s/it, lr=1.5e-5, step_loss=0.279]\n",
      "Steps:  70%|███████   | 7/10 [04:01<01:08, 22.67s/it, lr=1.5e-5, step_loss=0.232]\n",
      "Steps:  80%|████████  | 8/10 [04:03<00:41, 20.81s/it, lr=1.5e-5, step_loss=0.232]\n",
      "Steps:  80%|████████  | 8/10 [04:03<00:41, 20.81s/it, lr=1e-5, step_loss=0.29]   \n",
      "Steps:  80%|████████  | 8/10 [04:06<00:41, 20.81s/it, lr=1e-5, step_loss=0.243]\n",
      "Steps:  80%|████████  | 8/10 [04:08<00:41, 20.81s/it, lr=1e-5, step_loss=0.27] \n",
      "Steps:  80%|████████  | 8/10 [04:10<00:41, 20.81s/it, lr=1e-5, step_loss=0.277]\n",
      "Steps:  80%|████████  | 8/10 [04:12<00:41, 20.81s/it, lr=1e-5, step_loss=0.224]\n",
      "Steps:  80%|████████  | 8/10 [04:14<00:41, 20.81s/it, lr=1e-5, step_loss=0.278]\n",
      "Steps:  80%|████████  | 8/10 [04:16<00:41, 20.81s/it, lr=1e-5, step_loss=0.335]\n",
      "Steps:  80%|████████  | 8/10 [04:18<00:41, 20.81s/it, lr=1e-5, step_loss=0.317]\n",
      "Steps:  90%|█████████ | 9/10 [04:21<00:19, 19.68s/it, lr=1e-5, step_loss=0.317]\n",
      "Steps:  90%|█████████ | 9/10 [04:21<00:19, 19.68s/it, lr=5e-6, step_loss=0.234]\n",
      "Steps:  90%|█████████ | 9/10 [04:23<00:19, 19.68s/it, lr=5e-6, step_loss=0.234]\n",
      "Steps:  90%|█████████ | 9/10 [04:25<00:19, 19.68s/it, lr=5e-6, step_loss=0.253]\n",
      "Steps:  90%|█████████ | 9/10 [04:27<00:19, 19.68s/it, lr=5e-6, step_loss=0.312]\n",
      "Steps:  90%|█████████ | 9/10 [04:29<00:19, 19.68s/it, lr=5e-6, step_loss=0.218]\n",
      "Steps:  90%|█████████ | 9/10 [04:31<00:19, 19.68s/it, lr=5e-6, step_loss=0.254]\n",
      "Steps:  90%|█████████ | 9/10 [04:33<00:19, 19.68s/it, lr=5e-6, step_loss=0.249]\n",
      "Steps:  90%|█████████ | 9/10 [04:36<00:19, 19.68s/it, lr=5e-6, step_loss=0.258]\n",
      "Steps: 100%|██████████| 10/10 [04:38<00:00, 18.88s/it, lr=5e-6, step_loss=0.258]\n",
      "Steps: 100%|██████████| 10/10 [04:38<00:00, 18.88s/it, lr=0, step_loss=0.233]   train_loss: 0.267490\n",
      "train_loss: 0.255376\n",
      "train_loss: 0.253598\n",
      "train_loss: 0.258497\n",
      "train_loss: 0.253409\n",
      "train_loss: 0.251784\n",
      "train_loss: 0.267093\n",
      "train_loss: 0.263794\n",
      "train_loss: 0.272077\n",
      "train_loss: 0.251416\n",
      "\n",
      "\n",
      "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Fetching 11 files:   9%|▉         | 1/11 [00:00<00:01,  5.36it/s]\u001b[A\n",
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 58.82it/s]\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  33%|███▎      | 2/6 [00:06<00:12,  3.23s/it]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'use_quant_conv', 'force_upcast', 'shift_factor', 'latents_mean', 'scaling_factor', 'use_post_quant_conv', 'mid_block_add_attention', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  50%|█████     | 3/6 [00:06<00:05,  1.97s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "{'timestep_spacing', 'sample_max_value', 'thresholding', 'clip_sample_range', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:06<00:00,  1.13s/it]\n",
      "INFO:__main__:Running validation... \n",
      " Generating 4 images with prompt: A portrait of a slim white woman in her twenties, plain background..\n",
      "Model weights saved in optuna-sd-model_bs8_lr5e-05_schedlinear_acc8_0/pytorch_lora_weights.safetensors\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  33%|███▎      | 2/6 [00:00<00:01,  3.08it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'use_quant_conv', 'force_upcast', 'shift_factor', 'latents_mean', 'scaling_factor', 'use_post_quant_conv', 'mid_block_add_attention', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  50%|█████     | 3/6 [00:00<00:00,  3.87it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  67%|██████▋   | 4/6 [00:01<00:00,  2.79it/s]\u001b[A{'timestep_spacing', 'sample_max_value', 'thresholding', 'clip_sample_range', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'resnet_time_scale_shift', 'conv_out_kernel', 'class_embeddings_concat', 'resnet_out_scale_factor', 'time_embedding_act_fn', 'cross_attention_norm', 'mid_block_type', 'reverse_transformer_layers_per_block', 'time_embedding_type', 'projection_class_embeddings_input_dim', 'dropout', 'class_embed_type', 'transformer_layers_per_block', 'addition_embed_type_num_heads', 'addition_embed_type', 'mid_block_only_cross_attention', 'time_embedding_dim', 'encoder_hid_dim_type', 'addition_time_embed_dim', 'attention_type', 'resnet_skip_time_act', 'time_cond_proj_dim', 'encoder_hid_dim', 'conv_in_kernel', 'num_attention_heads', 'timestep_post_act'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:17<00:00,  4.42s/it]\u001b[A\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:17<00:00,  2.96s/it]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "INFO:__main__:Running validation... \n",
      " Generating 4 images with prompt: A portrait of a slim white woman in her twenties, plain background..\n",
      "\n",
      "Steps: 100%|██████████| 10/10 [06:46<00:00, 40.63s/it, lr=0, step_loss=0.233]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 17:06:37,442] Trial 0 finished with value: 0.26749 and parameters: {'learning_rate': 5e-05, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8}. Best is trial 0 with value: 0.26749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 0] train_loss: 0.26749\n",
      "0.26749\n",
      "\n",
      "🚀 [Trial 1] Commande:\n",
      "accelerate launch train_text_to_image_lora.py --pretrained_model_name_or_path=stabilityai/stable-diffusion-2-1 --train_data_dir=/content/drive/MyDrive/PFE/main/PFE/dataset/images --caption_column=prompt --dataloader_num_workers=8 --resolution=512 --train_batch_size=8 --max_train_steps=10 --max_grad_norm=1 --gradient_accumulation_steps=4 --num_train_epochs=2 --checkpointing_steps=150 --learning_rate=0.0001 --lr_scheduler=linear --lr_warmup_steps=0 --mixed_precision=fp16 --seed=1337 --allow_tf32 --use_8bit_adam --output_dir=optuna-sd-model_bs8_lr0.0001_schedlinear_acc4_1 --validation_prompt=A portrait of a slim white woman in her twenties, plain background.\n",
      "\n",
      "2025-05-15 17:06:53.934146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747328813.961459   13587 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747328813.970628   13587 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-15 17:06:54.032104: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:__main__:Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'clip_sample_range', 'sample_max_value', 'dynamic_thresholding_ratio', 'timestep_spacing', 'thresholding', 'variance_type', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "{'latents_std', 'use_post_quant_conv', 'scaling_factor', 'mid_block_add_attention', 'use_quant_conv', 'shift_factor', 'latents_mean', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'class_embed_type', 'mid_block_only_cross_attention', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'num_attention_heads', 'encoder_hid_dim', 'time_embedding_type', 'resnet_skip_time_act', 'mid_block_type', 'transformer_layers_per_block', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'addition_embed_type_num_heads', 'cross_attention_norm', 'conv_out_kernel', 'dropout', 'timestep_post_act', 'time_embedding_act_fn', 'time_embedding_dim', 'resnet_out_scale_factor', 'attention_type', 'time_cond_proj_dim', 'addition_time_embed_dim', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "INFO:__main__:***** Running training *****\n",
      "INFO:__main__:  Num examples = 10805\n",
      "INFO:__main__:  Num Epochs = 1\n",
      "INFO:__main__:  Instantaneous batch size per device = 8\n",
      "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "INFO:__main__:  Gradient Accumulation steps = 4\n",
      "INFO:__main__:  Total optimization steps = 10\n",
      "\n",
      "Steps:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Steps:   0%|          | 0/10 [00:09<?, ?it/s, lr=0.0001, step_loss=0.252]\n",
      "Steps:   0%|          | 0/10 [00:12<?, ?it/s, lr=0.0001, step_loss=0.316]\n",
      "Steps:   0%|          | 0/10 [00:14<?, ?it/s, lr=0.0001, step_loss=0.268]\n",
      "Steps:  10%|█         | 1/10 [00:16<02:26, 16.31s/it, lr=0.0001, step_loss=0.268]\n",
      "Steps:  10%|█         | 1/10 [00:23<02:26, 16.31s/it, lr=9e-5, step_loss=0.269]  \n",
      "Steps:  10%|█         | 1/10 [00:25<02:26, 16.31s/it, lr=9e-5, step_loss=0.281]\n",
      "Steps:  10%|█         | 1/10 [00:27<02:26, 16.31s/it, lr=9e-5, step_loss=0.214]\n",
      "Steps:  10%|█         | 1/10 [00:30<02:26, 16.31s/it, lr=9e-5, step_loss=0.268]\n",
      "Steps:  20%|██        | 2/10 [02:15<10:16, 77.02s/it, lr=9e-5, step_loss=0.268]\n",
      "Steps:  20%|██        | 2/10 [02:15<10:16, 77.02s/it, lr=8e-5, step_loss=0.271]\n",
      "Steps:  20%|██        | 2/10 [02:17<10:16, 77.02s/it, lr=8e-5, step_loss=0.234]\n",
      "Steps:  20%|██        | 2/10 [02:19<10:16, 77.02s/it, lr=8e-5, step_loss=0.274]\n",
      "Steps:  20%|██        | 2/10 [02:22<10:16, 77.02s/it, lr=8e-5, step_loss=0.194]\n",
      "Steps:  30%|███       | 3/10 [02:24<05:19, 45.63s/it, lr=8e-5, step_loss=0.194]\n",
      "Steps:  30%|███       | 3/10 [02:24<05:19, 45.63s/it, lr=7e-5, step_loss=0.225]\n",
      "Steps:  30%|███       | 3/10 [02:26<05:19, 45.63s/it, lr=7e-5, step_loss=0.278]\n",
      "Steps:  30%|███       | 3/10 [02:28<05:19, 45.63s/it, lr=7e-5, step_loss=0.276]\n",
      "Steps:  30%|███       | 3/10 [02:30<05:19, 45.63s/it, lr=7e-5, step_loss=0.308]\n",
      "Steps:  40%|████      | 4/10 [02:32<03:05, 30.91s/it, lr=7e-5, step_loss=0.308]\n",
      "Steps:  40%|████      | 4/10 [02:32<03:05, 30.91s/it, lr=6e-5, step_loss=0.251]\n",
      "Steps:  40%|████      | 4/10 [02:34<03:05, 30.91s/it, lr=6e-5, step_loss=0.243]\n",
      "Steps:  40%|████      | 4/10 [02:36<03:05, 30.91s/it, lr=6e-5, step_loss=0.265]\n",
      "Steps:  40%|████      | 4/10 [02:38<03:05, 30.91s/it, lr=6e-5, step_loss=0.222]\n",
      "Steps:  50%|█████     | 5/10 [02:40<01:54, 22.84s/it, lr=6e-5, step_loss=0.222]\n",
      "Steps:  50%|█████     | 5/10 [02:40<01:54, 22.84s/it, lr=5e-5, step_loss=0.228]\n",
      "Steps:  50%|█████     | 5/10 [02:43<01:54, 22.84s/it, lr=5e-5, step_loss=0.305]\n",
      "Steps:  50%|█████     | 5/10 [02:45<01:54, 22.84s/it, lr=5e-5, step_loss=0.268]\n",
      "Steps:  50%|█████     | 5/10 [02:47<01:54, 22.84s/it, lr=5e-5, step_loss=0.261]\n",
      "Steps:  60%|██████    | 6/10 [02:49<01:12, 18.01s/it, lr=5e-5, step_loss=0.261]\n",
      "Steps:  60%|██████    | 6/10 [02:49<01:12, 18.01s/it, lr=4e-5, step_loss=0.231]\n",
      "Steps:  60%|██████    | 6/10 [02:51<01:12, 18.01s/it, lr=4e-5, step_loss=0.293]\n",
      "Steps:  60%|██████    | 6/10 [02:53<01:12, 18.01s/it, lr=4e-5, step_loss=0.223]\n",
      "Steps:  60%|██████    | 6/10 [02:56<01:12, 18.01s/it, lr=4e-5, step_loss=0.28] \n",
      "Steps:  70%|███████   | 7/10 [02:58<00:44, 14.97s/it, lr=4e-5, step_loss=0.28]\n",
      "Steps:  70%|███████   | 7/10 [02:58<00:44, 14.97s/it, lr=3e-5, step_loss=0.245]\n",
      "Steps:  70%|███████   | 7/10 [03:00<00:44, 14.97s/it, lr=3e-5, step_loss=0.233]\n",
      "Steps:  70%|███████   | 7/10 [03:02<00:44, 14.97s/it, lr=3e-5, step_loss=0.307]\n",
      "Steps:  70%|███████   | 7/10 [03:04<00:44, 14.97s/it, lr=3e-5, step_loss=0.26] \n",
      "Steps:  80%|████████  | 8/10 [03:06<00:25, 12.94s/it, lr=3e-5, step_loss=0.26]\n",
      "Steps:  80%|████████  | 8/10 [03:06<00:25, 12.94s/it, lr=2e-5, step_loss=0.22]\n",
      "Steps:  80%|████████  | 8/10 [03:09<00:25, 12.94s/it, lr=2e-5, step_loss=0.291]\n",
      "Steps:  80%|████████  | 8/10 [03:11<00:25, 12.94s/it, lr=2e-5, step_loss=0.27] \n",
      "Steps:  80%|████████  | 8/10 [03:13<00:25, 12.94s/it, lr=2e-5, step_loss=0.226]\n",
      "Steps:  90%|█████████ | 9/10 [03:15<00:11, 11.54s/it, lr=2e-5, step_loss=0.226]\n",
      "Steps:  90%|█████████ | 9/10 [03:15<00:11, 11.54s/it, lr=1e-5, step_loss=0.267]\n",
      "Steps:  90%|█████████ | 9/10 [03:17<00:11, 11.54s/it, lr=1e-5, step_loss=0.23] \n",
      "Steps:  90%|█████████ | 9/10 [03:19<00:11, 11.54s/it, lr=1e-5, step_loss=0.215]\n",
      "Steps:  90%|█████████ | 9/10 [03:21<00:11, 11.54s/it, lr=1e-5, step_loss=0.259]\n",
      "Steps: 100%|██████████| 10/10 [03:23<00:00, 10.56s/it, lr=1e-5, step_loss=0.259]\n",
      "Steps: 100%|██████████| 10/10 [03:23<00:00, 10.56s/it, lr=0, step_loss=0.263]   {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "train_loss: 0.276275\n",
      "train_loss: 0.258538\n",
      "train_loss: 0.231677\n",
      "train_loss: 0.278326\n",
      "train_loss: 0.239495\n",
      "train_loss: 0.266270\n",
      "train_loss: 0.260388\n",
      "train_loss: 0.254889\n",
      "train_loss: 0.263518\n",
      "train_loss: 0.241547\n",
      "\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  17%|█▋        | 1/6 [00:01<00:09,  1.85s/it]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_std', 'use_post_quant_conv', 'scaling_factor', 'mid_block_add_attention', 'use_quant_conv', 'shift_factor', 'latents_mean', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  33%|███▎      | 2/6 [00:02<00:04,  1.14s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "{'clip_sample_range', 'sample_max_value', 'dynamic_thresholding_ratio', 'timestep_spacing', 'thresholding', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:02<00:00,  2.33it/s]\n",
      "INFO:__main__:Running validation... \n",
      " Generating 4 images with prompt: A portrait of a slim white woman in her twenties, plain background..\n",
      "Model weights saved in optuna-sd-model_bs8_lr0.0001_schedlinear_acc4_1/pytorch_lora_weights.safetensors\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  17%|█▋        | 1/6 [00:00<00:03,  1.33it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'latents_std', 'use_post_quant_conv', 'scaling_factor', 'mid_block_add_attention', 'use_quant_conv', 'shift_factor', 'latents_mean', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  33%|███▎      | 2/6 [00:00<00:01,  2.48it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  50%|█████     | 3/6 [00:01<00:01,  2.11it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'class_embed_type', 'mid_block_only_cross_attention', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'num_attention_heads', 'encoder_hid_dim', 'time_embedding_type', 'resnet_skip_time_act', 'mid_block_type', 'transformer_layers_per_block', 'class_embeddings_concat', 'reverse_transformer_layers_per_block', 'addition_embed_type_num_heads', 'cross_attention_norm', 'conv_out_kernel', 'dropout', 'timestep_post_act', 'time_embedding_act_fn', 'time_embedding_dim', 'resnet_out_scale_factor', 'attention_type', 'time_cond_proj_dim', 'addition_time_embed_dim', 'encoder_hid_dim_type', 'resnet_time_scale_shift', 'addition_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  67%|██████▋   | 4/6 [00:17<00:13,  6.57s/it]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "{'clip_sample_range', 'sample_max_value', 'dynamic_thresholding_ratio', 'timestep_spacing', 'thresholding', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:17<00:00,  2.90s/it]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "INFO:__main__:Running validation... \n",
      " Generating 4 images with prompt: A portrait of a slim white woman in her twenties, plain background..\n",
      "\n",
      "Steps: 100%|██████████| 10/10 [05:26<00:00, 32.66s/it, lr=0, step_loss=0.263]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 17:13:14,516] Trial 1 finished with value: 0.276275 and parameters: {'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.26749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 1] train_loss: 0.276275\n",
      "0.276275\n",
      "\n",
      "🚀 [Trial 2] Commande:\n",
      "accelerate launch train_text_to_image_lora.py --pretrained_model_name_or_path=stabilityai/stable-diffusion-2-1 --train_data_dir=/content/drive/MyDrive/PFE/main/PFE/dataset/images --caption_column=prompt --dataloader_num_workers=8 --resolution=512 --train_batch_size=8 --max_train_steps=10 --max_grad_norm=1 --gradient_accumulation_steps=8 --num_train_epochs=2 --checkpointing_steps=150 --learning_rate=5e-05 --lr_scheduler=linear --lr_warmup_steps=0 --mixed_precision=fp16 --seed=1337 --allow_tf32 --use_8bit_adam --output_dir=optuna-sd-model_bs8_lr5e-05_schedlinear_acc8_2 --validation_prompt=A portrait of a slim white woman in her twenties, plain background.\n",
      "\n",
      "2025-05-15 17:13:35.684771: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747329215.874605   15384 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747329215.928624   15384 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-15 17:13:36.362379: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:__main__:Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'timestep_spacing', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'variance_type', 'sample_max_value', 'thresholding', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_add_attention', 'latents_std', 'latents_mean', 'scaling_factor', 'shift_factor', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'num_attention_heads', 'class_embeddings_concat', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'conv_in_kernel', 'encoder_hid_dim', 'time_embedding_dim', 'addition_embed_type_num_heads', 'cross_attention_norm', 'time_embedding_act_fn', 'reverse_transformer_layers_per_block', 'mid_block_type', 'time_cond_proj_dim', 'addition_embed_type', 'class_embed_type', 'dropout', 'time_embedding_type', 'attention_type', 'resnet_skip_time_act', 'resnet_time_scale_shift', 'encoder_hid_dim_type', 'transformer_layers_per_block', 'timestep_post_act', 'conv_out_kernel', 'projection_class_embeddings_input_dim', 'addition_time_embed_dim'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "INFO:__main__:***** Running training *****\n",
      "INFO:__main__:  Num examples = 10805\n",
      "INFO:__main__:  Num Epochs = 1\n",
      "INFO:__main__:  Instantaneous batch size per device = 8\n",
      "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "INFO:__main__:  Gradient Accumulation steps = 8\n",
      "INFO:__main__:  Total optimization steps = 10\n",
      "\n",
      "Steps:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Steps:   0%|          | 0/10 [00:09<?, ?it/s, lr=5e-5, step_loss=0.252]\n",
      "Steps:   0%|          | 0/10 [00:11<?, ?it/s, lr=5e-5, step_loss=0.316]\n",
      "Steps:   0%|          | 0/10 [00:13<?, ?it/s, lr=5e-5, step_loss=0.268]\n",
      "Steps:   0%|          | 0/10 [00:15<?, ?it/s, lr=5e-5, step_loss=0.269]\n",
      "Steps:   0%|          | 0/10 [00:17<?, ?it/s, lr=5e-5, step_loss=0.281]\n",
      "Steps:   0%|          | 0/10 [00:21<?, ?it/s, lr=5e-5, step_loss=0.214]\n",
      "Steps:   0%|          | 0/10 [00:23<?, ?it/s, lr=5e-5, step_loss=0.268]\n",
      "Steps:  10%|█         | 1/10 [00:25<03:49, 25.53s/it, lr=5e-5, step_loss=0.268]\n",
      "Steps:  10%|█         | 1/10 [00:32<03:49, 25.53s/it, lr=4.5e-5, step_loss=0.271]\n",
      "Steps:  10%|█         | 1/10 [00:34<03:49, 25.53s/it, lr=4.5e-5, step_loss=0.234]\n",
      "Steps:  10%|█         | 1/10 [00:36<03:49, 25.53s/it, lr=4.5e-5, step_loss=0.274]\n",
      "Steps:  10%|█         | 1/10 [00:38<03:49, 25.53s/it, lr=4.5e-5, step_loss=0.194]\n",
      "Steps:  10%|█         | 1/10 [00:40<03:49, 25.53s/it, lr=4.5e-5, step_loss=0.225]\n",
      "Steps:  10%|█         | 1/10 [00:42<03:49, 25.53s/it, lr=4.5e-5, step_loss=0.279]\n",
      "Steps:  10%|█         | 1/10 [00:44<03:49, 25.53s/it, lr=4.5e-5, step_loss=0.276]\n",
      "Steps:  10%|█         | 1/10 [00:47<03:49, 25.53s/it, lr=4.5e-5, step_loss=0.309]\n",
      "Steps:  20%|██        | 2/10 [02:12<09:49, 73.65s/it, lr=4.5e-5, step_loss=0.309]\n",
      "Steps:  20%|██        | 2/10 [02:12<09:49, 73.65s/it, lr=4e-5, step_loss=0.252]  \n",
      "Steps:  20%|██        | 2/10 [02:14<09:49, 73.65s/it, lr=4e-5, step_loss=0.243]\n",
      "Steps:  20%|██        | 2/10 [02:17<09:49, 73.65s/it, lr=4e-5, step_loss=0.266]\n",
      "Steps:  20%|██        | 2/10 [02:19<09:49, 73.65s/it, lr=4e-5, step_loss=0.223]\n",
      "Steps:  20%|██        | 2/10 [02:21<09:49, 73.65s/it, lr=4e-5, step_loss=0.228]\n",
      "Steps:  20%|██        | 2/10 [02:23<09:49, 73.65s/it, lr=4e-5, step_loss=0.306]\n",
      "Steps:  20%|██        | 2/10 [02:25<09:49, 73.65s/it, lr=4e-5, step_loss=0.27] \n",
      "Steps:  20%|██        | 2/10 [02:27<09:49, 73.65s/it, lr=4e-5, step_loss=0.262]\n",
      "Steps:  30%|███       | 3/10 [02:29<05:33, 47.68s/it, lr=4e-5, step_loss=0.262]\n",
      "Steps:  30%|███       | 3/10 [02:29<05:33, 47.68s/it, lr=3.5e-5, step_loss=0.231]\n",
      "Steps:  30%|███       | 3/10 [02:31<05:33, 47.68s/it, lr=3.5e-5, step_loss=0.294]\n",
      "Steps:  30%|███       | 3/10 [02:33<05:33, 47.68s/it, lr=3.5e-5, step_loss=0.224]\n",
      "Steps:  30%|███       | 3/10 [02:36<05:33, 47.68s/it, lr=3.5e-5, step_loss=0.281]\n",
      "Steps:  30%|███       | 3/10 [02:38<05:33, 47.68s/it, lr=3.5e-5, step_loss=0.246]\n",
      "Steps:  30%|███       | 3/10 [02:40<05:33, 47.68s/it, lr=3.5e-5, step_loss=0.233]\n",
      "Steps:  30%|███       | 3/10 [02:42<05:33, 47.68s/it, lr=3.5e-5, step_loss=0.308]\n",
      "Steps:  30%|███       | 3/10 [02:44<05:33, 47.68s/it, lr=3.5e-5, step_loss=0.261]\n",
      "Steps:  40%|████      | 4/10 [02:46<03:34, 35.71s/it, lr=3.5e-5, step_loss=0.261]\n",
      "Steps:  40%|████      | 4/10 [02:46<03:34, 35.71s/it, lr=3e-5, step_loss=0.221]  \n",
      "Steps:  40%|████      | 4/10 [02:49<03:34, 35.71s/it, lr=3e-5, step_loss=0.292]\n",
      "Steps:  40%|████      | 4/10 [02:51<03:34, 35.71s/it, lr=3e-5, step_loss=0.271]\n",
      "Steps:  40%|████      | 4/10 [02:53<03:34, 35.71s/it, lr=3e-5, step_loss=0.227]\n",
      "Steps:  40%|████      | 4/10 [02:55<03:34, 35.71s/it, lr=3e-5, step_loss=0.267]\n",
      "Steps:  40%|████      | 4/10 [02:57<03:34, 35.71s/it, lr=3e-5, step_loss=0.23] \n",
      "Steps:  40%|████      | 4/10 [02:59<03:34, 35.71s/it, lr=3e-5, step_loss=0.216]\n",
      "Steps:  40%|████      | 4/10 [03:01<03:34, 35.71s/it, lr=3e-5, step_loss=0.259]\n",
      "Steps:  50%|█████     | 5/10 [03:04<02:25, 29.01s/it, lr=3e-5, step_loss=0.259]\n",
      "Steps:  50%|█████     | 5/10 [03:04<02:25, 29.01s/it, lr=2.5e-5, step_loss=0.263]\n",
      "Steps:  50%|█████     | 5/10 [03:06<02:25, 29.01s/it, lr=2.5e-5, step_loss=0.243]\n",
      "Steps:  50%|█████     | 5/10 [03:08<02:25, 29.01s/it, lr=2.5e-5, step_loss=0.232]\n",
      "Steps:  50%|█████     | 5/10 [03:10<02:25, 29.01s/it, lr=2.5e-5, step_loss=0.219]\n",
      "Steps:  50%|█████     | 5/10 [03:12<02:25, 29.01s/it, lr=2.5e-5, step_loss=0.289]\n",
      "Steps:  50%|█████     | 5/10 [03:14<02:25, 29.01s/it, lr=2.5e-5, step_loss=0.293]\n",
      "Steps:  50%|█████     | 5/10 [03:16<02:25, 29.01s/it, lr=2.5e-5, step_loss=0.246]\n",
      "Steps:  50%|█████     | 5/10 [03:18<02:25, 29.01s/it, lr=2.5e-5, step_loss=0.246]\n",
      "Steps:  60%|██████    | 6/10 [03:20<01:39, 24.85s/it, lr=2.5e-5, step_loss=0.246]\n",
      "Steps:  60%|██████    | 6/10 [03:20<01:39, 24.85s/it, lr=2e-5, step_loss=0.246]  \n",
      "Steps:  60%|██████    | 6/10 [03:22<01:39, 24.85s/it, lr=2e-5, step_loss=0.26] \n",
      "Steps:  60%|██████    | 6/10 [03:25<01:39, 24.85s/it, lr=2e-5, step_loss=0.269]\n",
      "Steps:  60%|██████    | 6/10 [03:27<01:39, 24.85s/it, lr=2e-5, step_loss=0.275]\n",
      "Steps:  60%|██████    | 6/10 [03:29<01:39, 24.85s/it, lr=2e-5, step_loss=0.214]\n",
      "Steps:  60%|██████    | 6/10 [03:31<01:39, 24.85s/it, lr=2e-5, step_loss=0.295]\n",
      "Steps:  60%|██████    | 6/10 [03:33<01:39, 24.85s/it, lr=2e-5, step_loss=0.287]\n",
      "Steps:  60%|██████    | 6/10 [03:35<01:39, 24.85s/it, lr=2e-5, step_loss=0.294]\n",
      "Steps:  70%|███████   | 7/10 [03:37<01:06, 22.16s/it, lr=2e-5, step_loss=0.294]\n",
      "Steps:  70%|███████   | 7/10 [03:37<01:06, 22.16s/it, lr=1.5e-5, step_loss=0.243]\n",
      "Steps:  70%|███████   | 7/10 [03:39<01:06, 22.16s/it, lr=1.5e-5, step_loss=0.275]\n",
      "Steps:  70%|███████   | 7/10 [03:41<01:06, 22.16s/it, lr=1.5e-5, step_loss=0.314]\n",
      "Steps:  70%|███████   | 7/10 [03:43<01:06, 22.16s/it, lr=1.5e-5, step_loss=0.24] \n",
      "Steps:  70%|███████   | 7/10 [03:45<01:06, 22.16s/it, lr=1.5e-5, step_loss=0.228]\n",
      "Steps:  70%|███████   | 7/10 [03:47<01:06, 22.16s/it, lr=1.5e-5, step_loss=0.252]\n",
      "Steps:  70%|███████   | 7/10 [03:50<01:06, 22.16s/it, lr=1.5e-5, step_loss=0.279]\n",
      "Steps:  70%|███████   | 7/10 [03:52<01:06, 22.16s/it, lr=1.5e-5, step_loss=0.232]\n",
      "Steps:  80%|████████  | 8/10 [03:54<00:40, 20.44s/it, lr=1.5e-5, step_loss=0.232]\n",
      "Steps:  80%|████████  | 8/10 [03:54<00:40, 20.44s/it, lr=1e-5, step_loss=0.29]   \n",
      "Steps:  80%|████████  | 8/10 [03:56<00:40, 20.44s/it, lr=1e-5, step_loss=0.243]\n",
      "Steps:  80%|████████  | 8/10 [03:58<00:40, 20.44s/it, lr=1e-5, step_loss=0.27] \n",
      "Steps:  80%|████████  | 8/10 [04:00<00:40, 20.44s/it, lr=1e-5, step_loss=0.277]\n",
      "Steps:  80%|████████  | 8/10 [04:02<00:40, 20.44s/it, lr=1e-5, step_loss=0.224]\n",
      "Steps:  80%|████████  | 8/10 [04:04<00:40, 20.44s/it, lr=1e-5, step_loss=0.278]\n",
      "Steps:  80%|████████  | 8/10 [04:06<00:40, 20.44s/it, lr=1e-5, step_loss=0.335]\n",
      "Steps:  80%|████████  | 8/10 [04:09<00:40, 20.44s/it, lr=1e-5, step_loss=0.317]\n",
      "Steps:  90%|█████████ | 9/10 [04:11<00:19, 19.35s/it, lr=1e-5, step_loss=0.317]\n",
      "Steps:  90%|█████████ | 9/10 [04:11<00:19, 19.35s/it, lr=5e-6, step_loss=0.234]\n",
      "Steps:  90%|█████████ | 9/10 [04:13<00:19, 19.35s/it, lr=5e-6, step_loss=0.234]\n",
      "Steps:  90%|█████████ | 9/10 [04:15<00:19, 19.35s/it, lr=5e-6, step_loss=0.253]\n",
      "Steps:  90%|█████████ | 9/10 [04:17<00:19, 19.35s/it, lr=5e-6, step_loss=0.312]\n",
      "Steps:  90%|█████████ | 9/10 [04:19<00:19, 19.35s/it, lr=5e-6, step_loss=0.218]\n",
      "Steps:  90%|█████████ | 9/10 [04:21<00:19, 19.35s/it, lr=5e-6, step_loss=0.254]\n",
      "Steps:  90%|█████████ | 9/10 [04:23<00:19, 19.35s/it, lr=5e-6, step_loss=0.249]\n",
      "Steps:  90%|█████████ | 9/10 [04:26<00:19, 19.35s/it, lr=5e-6, step_loss=0.259]\n",
      "Steps: 100%|██████████| 10/10 [04:28<00:00, 18.62s/it, lr=5e-6, step_loss=0.259]\n",
      "Steps: 100%|██████████| 10/10 [04:28<00:00, 18.62s/it, lr=0, step_loss=0.233]   {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "train_loss: 0.267490\n",
      "train_loss: 0.255369\n",
      "train_loss: 0.253591\n",
      "train_loss: 0.258483\n",
      "train_loss: 0.253413\n",
      "train_loss: 0.251784\n",
      "train_loss: 0.267107\n",
      "train_loss: 0.263772\n",
      "train_loss: 0.272081\n",
      "train_loss: 0.251426\n",
      "\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "{'timestep_spacing', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'sample_max_value', 'thresholding', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  83%|████████▎ | 5/6 [00:07<00:01,  1.41s/it]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'mid_block_add_attention', 'latents_std', 'latents_mean', 'scaling_factor', 'shift_factor', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:08<00:00,  1.40s/it]\u001b[A\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:08<00:00,  1.40s/it]\n",
      "INFO:__main__:Running validation... \n",
      " Generating 4 images with prompt: A portrait of a slim white woman in her twenties, plain background..\n",
      "Model weights saved in optuna-sd-model_bs8_lr5e-05_schedlinear_acc8_2/pytorch_lora_weights.safetensors\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "{'timestep_spacing', 'dynamic_thresholding_ratio', 'rescale_betas_zero_snr', 'sample_max_value', 'thresholding', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'num_attention_heads', 'class_embeddings_concat', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'conv_in_kernel', 'encoder_hid_dim', 'time_embedding_dim', 'addition_embed_type_num_heads', 'cross_attention_norm', 'time_embedding_act_fn', 'reverse_transformer_layers_per_block', 'mid_block_type', 'time_cond_proj_dim', 'addition_embed_type', 'class_embed_type', 'dropout', 'time_embedding_type', 'attention_type', 'resnet_skip_time_act', 'resnet_time_scale_shift', 'encoder_hid_dim_type', 'transformer_layers_per_block', 'timestep_post_act', 'conv_out_kernel', 'projection_class_embeddings_input_dim', 'addition_time_embed_dim'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  67%|██████▋   | 4/6 [00:12<00:06,  3.21s/it]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  83%|████████▎ | 5/6 [00:19<00:04,  4.17s/it]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'mid_block_add_attention', 'latents_std', 'latents_mean', 'scaling_factor', 'shift_factor', 'force_upcast', 'use_post_quant_conv', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:21<00:00,  3.44s/it]\u001b[A\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:21<00:00,  3.53s/it]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "INFO:__main__:Running validation... \n",
      " Generating 4 images with prompt: A portrait of a slim white woman in her twenties, plain background..\n",
      "\n",
      "Steps: 100%|██████████| 10/10 [06:42<00:00, 40.21s/it, lr=0, step_loss=0.233]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 17:21:00,565] Trial 2 finished with value: 0.26749 and parameters: {'learning_rate': 5e-05, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8}. Best is trial 0 with value: 0.26749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 2] train_loss: 0.26749\n",
      "0.26749\n",
      "\n",
      "🚀 [Trial 3] Commande:\n",
      "accelerate launch train_text_to_image_lora.py --pretrained_model_name_or_path=stabilityai/stable-diffusion-2-1 --train_data_dir=/content/drive/MyDrive/PFE/main/PFE/dataset/images --caption_column=prompt --dataloader_num_workers=8 --resolution=512 --train_batch_size=8 --max_train_steps=10 --max_grad_norm=1 --gradient_accumulation_steps=4 --num_train_epochs=2 --checkpointing_steps=150 --learning_rate=5e-05 --lr_scheduler=linear --lr_warmup_steps=0 --mixed_precision=fp16 --seed=1337 --allow_tf32 --use_8bit_adam --output_dir=optuna-sd-model_bs8_lr5e-05_schedlinear_acc4_3 --validation_prompt=A portrait of a slim white woman in her twenties, plain background.\n",
      "\n",
      "2025-05-15 17:21:21.157481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747329681.186827   17453 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747329681.195481   17453 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-15 17:21:21.227995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:__main__:Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'sample_max_value', 'timestep_spacing', 'dynamic_thresholding_ratio', 'clip_sample_range', 'variance_type', 'thresholding', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "{'force_upcast', 'mid_block_add_attention', 'use_quant_conv', 'scaling_factor', 'use_post_quant_conv', 'shift_factor', 'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'addition_embed_type', 'timestep_post_act', 'mid_block_only_cross_attention', 'encoder_hid_dim', 'reverse_transformer_layers_per_block', 'resnet_out_scale_factor', 'time_embedding_type', 'cross_attention_norm', 'addition_embed_type_num_heads', 'encoder_hid_dim_type', 'addition_time_embed_dim', 'dropout', 'time_embedding_act_fn', 'resnet_skip_time_act', 'time_cond_proj_dim', 'resnet_time_scale_shift', 'class_embed_type', 'mid_block_type', 'conv_in_kernel', 'class_embeddings_concat', 'conv_out_kernel', 'transformer_layers_per_block', 'projection_class_embeddings_input_dim', 'attention_type', 'time_embedding_dim', 'num_attention_heads'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "INFO:__main__:***** Running training *****\n",
      "INFO:__main__:  Num examples = 10805\n",
      "INFO:__main__:  Num Epochs = 1\n",
      "INFO:__main__:  Instantaneous batch size per device = 8\n",
      "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "INFO:__main__:  Gradient Accumulation steps = 4\n",
      "INFO:__main__:  Total optimization steps = 10\n",
      "\n",
      "Steps:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Steps:   0%|          | 0/10 [00:09<?, ?it/s, lr=5e-5, step_loss=0.252]\n",
      "Steps:   0%|          | 0/10 [00:11<?, ?it/s, lr=5e-5, step_loss=0.316]\n",
      "Steps:   0%|          | 0/10 [00:13<?, ?it/s, lr=5e-5, step_loss=0.268]\n",
      "Steps:  10%|█         | 1/10 [00:15<02:19, 15.45s/it, lr=5e-5, step_loss=0.268]\n",
      "Steps:  10%|█         | 1/10 [00:22<02:19, 15.45s/it, lr=4.5e-5, step_loss=0.269]\n",
      "Steps:  10%|█         | 1/10 [00:24<02:19, 15.45s/it, lr=4.5e-5, step_loss=0.281]\n",
      "Steps:  10%|█         | 1/10 [00:26<02:19, 15.45s/it, lr=4.5e-5, step_loss=0.214]\n",
      "Steps:  10%|█         | 1/10 [00:28<02:19, 15.45s/it, lr=4.5e-5, step_loss=0.268]\n",
      "Steps:  20%|██        | 2/10 [01:05<04:46, 35.82s/it, lr=4.5e-5, step_loss=0.268]\n",
      "Steps:  20%|██        | 2/10 [01:10<04:46, 35.82s/it, lr=4e-5, step_loss=0.271]  \n",
      "Steps:  20%|██        | 2/10 [01:12<04:46, 35.82s/it, lr=4e-5, step_loss=0.234]\n",
      "Steps:  20%|██        | 2/10 [01:14<04:46, 35.82s/it, lr=4e-5, step_loss=0.274]\n",
      "Steps:  20%|██        | 2/10 [01:17<04:46, 35.82s/it, lr=4e-5, step_loss=0.194]\n",
      "Steps:  30%|███       | 3/10 [01:19<02:59, 25.69s/it, lr=4e-5, step_loss=0.194]\n",
      "Steps:  30%|███       | 3/10 [01:23<02:59, 25.69s/it, lr=3.5e-5, step_loss=0.225]\n",
      "Steps:  30%|███       | 3/10 [02:01<02:59, 25.69s/it, lr=3.5e-5, step_loss=0.278]\n",
      "Steps:  30%|███       | 3/10 [02:03<02:59, 25.69s/it, lr=3.5e-5, step_loss=0.276]\n",
      "Steps:  30%|███       | 3/10 [02:05<02:59, 25.69s/it, lr=3.5e-5, step_loss=0.308]\n",
      "Steps:  40%|████      | 4/10 [02:07<03:28, 34.77s/it, lr=3.5e-5, step_loss=0.308]\n",
      "Steps:  40%|████      | 4/10 [02:07<03:28, 34.77s/it, lr=3e-5, step_loss=0.252]  \n",
      "Steps:  40%|████      | 4/10 [02:10<03:28, 34.77s/it, lr=3e-5, step_loss=0.243]\n",
      "Steps:  40%|████      | 4/10 [02:12<03:28, 34.77s/it, lr=3e-5, step_loss=0.266]\n",
      "Steps:  40%|████      | 4/10 [02:14<03:28, 34.77s/it, lr=3e-5, step_loss=0.223]\n",
      "Steps:  50%|█████     | 5/10 [02:16<02:06, 25.36s/it, lr=3e-5, step_loss=0.223]\n",
      "Steps:  50%|█████     | 5/10 [02:16<02:06, 25.36s/it, lr=2.5e-5, step_loss=0.228]\n",
      "Steps:  50%|█████     | 5/10 [02:18<02:06, 25.36s/it, lr=2.5e-5, step_loss=0.305]\n",
      "Steps:  50%|█████     | 5/10 [02:20<02:06, 25.36s/it, lr=2.5e-5, step_loss=0.269]\n",
      "Steps:  50%|█████     | 5/10 [02:23<02:06, 25.36s/it, lr=2.5e-5, step_loss=0.262]\n",
      "Steps:  60%|██████    | 6/10 [02:25<01:18, 19.73s/it, lr=2.5e-5, step_loss=0.262]\n",
      "Steps:  60%|██████    | 6/10 [02:25<01:18, 19.73s/it, lr=2e-5, step_loss=0.231]  \n",
      "Steps:  60%|██████    | 6/10 [02:27<01:18, 19.73s/it, lr=2e-5, step_loss=0.294]\n",
      "Steps:  60%|██████    | 6/10 [02:29<01:18, 19.73s/it, lr=2e-5, step_loss=0.224]\n",
      "Steps:  60%|██████    | 6/10 [02:31<01:18, 19.73s/it, lr=2e-5, step_loss=0.28] \n",
      "Steps:  70%|███████   | 7/10 [02:33<00:48, 16.11s/it, lr=2e-5, step_loss=0.28]\n",
      "Steps:  70%|███████   | 7/10 [02:33<00:48, 16.11s/it, lr=1.5e-5, step_loss=0.246]\n",
      "Steps:  70%|███████   | 7/10 [02:36<00:48, 16.11s/it, lr=1.5e-5, step_loss=0.233]\n",
      "Steps:  70%|███████   | 7/10 [02:38<00:48, 16.11s/it, lr=1.5e-5, step_loss=0.307]\n",
      "Steps:  70%|███████   | 7/10 [02:40<00:48, 16.11s/it, lr=1.5e-5, step_loss=0.261]\n",
      "Steps:  80%|████████  | 8/10 [02:42<00:27, 13.68s/it, lr=1.5e-5, step_loss=0.261]\n",
      "Steps:  80%|████████  | 8/10 [02:42<00:27, 13.68s/it, lr=1e-5, step_loss=0.221]  \n",
      "Steps:  80%|████████  | 8/10 [02:44<00:27, 13.68s/it, lr=1e-5, step_loss=0.292]\n",
      "Steps:  80%|████████  | 8/10 [02:46<00:27, 13.68s/it, lr=1e-5, step_loss=0.271]\n",
      "Steps:  80%|████████  | 8/10 [02:48<00:27, 13.68s/it, lr=1e-5, step_loss=0.227]\n",
      "Steps:  90%|█████████ | 9/10 [02:50<00:12, 12.03s/it, lr=1e-5, step_loss=0.227]\n",
      "Steps:  90%|█████████ | 9/10 [02:50<00:12, 12.03s/it, lr=5e-6, step_loss=0.267]\n",
      "Steps:  90%|█████████ | 9/10 [02:52<00:12, 12.03s/it, lr=5e-6, step_loss=0.23] \n",
      "Steps:  90%|█████████ | 9/10 [02:55<00:12, 12.03s/it, lr=5e-6, step_loss=0.216]\n",
      "Steps:  90%|█████████ | 9/10 [02:57<00:12, 12.03s/it, lr=5e-6, step_loss=0.259]\n",
      "Steps: 100%|██████████| 10/10 [02:59<00:00, 10.88s/it, lr=5e-6, step_loss=0.259]\n",
      "Steps: 100%|██████████| 10/10 [02:59<00:00, 10.88s/it, lr=0, step_loss=0.263]   {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "train_loss: 0.276275\n",
      "train_loss: 0.258613\n",
      "train_loss: 0.231853\n",
      "train_loss: 0.278637\n",
      "train_loss: 0.239994\n",
      "train_loss: 0.266826\n",
      "train_loss: 0.260975\n",
      "train_loss: 0.255610\n",
      "train_loss: 0.264305\n",
      "train_loss: 0.242243\n",
      "\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A{'sample_max_value', 'timestep_spacing', 'dynamic_thresholding_ratio', 'clip_sample_range', 'thresholding', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'force_upcast', 'mid_block_add_attention', 'use_quant_conv', 'scaling_factor', 'use_post_quant_conv', 'shift_factor', 'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  33%|███▎      | 2/6 [00:01<00:02,  1.96it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  50%|█████     | 3/6 [00:02<00:02,  1.02it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:02<00:00,  2.19it/s]\n",
      "INFO:__main__:Running validation... \n",
      " Generating 4 images with prompt: A portrait of a slim white woman in her twenties, plain background..\n",
      "Model weights saved in optuna-sd-model_bs8_lr5e-05_schedlinear_acc4_3/pytorch_lora_weights.safetensors\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A{'sample_max_value', 'timestep_spacing', 'dynamic_thresholding_ratio', 'clip_sample_range', 'thresholding', 'rescale_betas_zero_snr'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'force_upcast', 'mid_block_add_attention', 'use_quant_conv', 'scaling_factor', 'use_post_quant_conv', 'shift_factor', 'latents_mean', 'latents_std'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  33%|███▎      | 2/6 [00:00<00:00, 13.92it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  67%|██████▋   | 4/6 [00:00<00:00,  4.43it/s]\u001b[AInstantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'addition_embed_type', 'timestep_post_act', 'mid_block_only_cross_attention', 'encoder_hid_dim', 'reverse_transformer_layers_per_block', 'resnet_out_scale_factor', 'time_embedding_type', 'cross_attention_norm', 'addition_embed_type_num_heads', 'encoder_hid_dim_type', 'addition_time_embed_dim', 'dropout', 'time_embedding_act_fn', 'resnet_skip_time_act', 'time_cond_proj_dim', 'resnet_time_scale_shift', 'class_embed_type', 'mid_block_type', 'conv_in_kernel', 'class_embeddings_concat', 'conv_out_kernel', 'transformer_layers_per_block', 'projection_class_embeddings_input_dim', 'attention_type', 'time_embedding_dim', 'num_attention_heads'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  83%|████████▎ | 5/6 [00:16<00:04,  4.84s/it]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:16<00:00,  2.78s/it]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "INFO:__main__:Running validation... \n",
      " Generating 4 images with prompt: A portrait of a slim white woman in her twenties, plain background..\n",
      "\n",
      "Steps: 100%|██████████| 10/10 [05:01<00:00, 30.19s/it, lr=0, step_loss=0.263]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 17:27:00,846] Trial 3 finished with value: 0.276275 and parameters: {'learning_rate': 5e-05, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.26749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 3] train_loss: 0.276275\n",
      "0.276275\n",
      "\n",
      "🚀 [Trial 4] Commande:\n",
      "accelerate launch train_text_to_image_lora.py --pretrained_model_name_or_path=stabilityai/stable-diffusion-2-1 --train_data_dir=/content/drive/MyDrive/PFE/main/PFE/dataset/images --caption_column=prompt --dataloader_num_workers=8 --resolution=512 --train_batch_size=8 --max_train_steps=10 --max_grad_norm=1 --gradient_accumulation_steps=4 --num_train_epochs=2 --checkpointing_steps=150 --learning_rate=1e-05 --lr_scheduler=linear --lr_warmup_steps=0 --mixed_precision=fp16 --seed=1337 --allow_tf32 --use_8bit_adam --output_dir=optuna-sd-model_bs8_lr1e-05_schedlinear_acc4_4 --validation_prompt=A portrait of a slim white woman in her twenties, plain background.\n",
      "\n",
      "2025-05-15 17:27:18.163564: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747330038.192273   19046 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747330038.200610   19046 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-15 17:27:18.234054: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO:__main__:Distributed environment: DistributedType.NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "{'sample_max_value', 'rescale_betas_zero_snr', 'thresholding', 'clip_sample_range', 'variance_type', 'timestep_spacing', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "{'mid_block_add_attention', 'shift_factor', 'force_upcast', 'use_post_quant_conv', 'scaling_factor', 'latents_std', 'latents_mean', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "{'conv_in_kernel', 'encoder_hid_dim', 'num_attention_heads', 'mid_block_only_cross_attention', 'time_cond_proj_dim', 'time_embedding_type', 'encoder_hid_dim_type', 'time_embedding_act_fn', 'mid_block_type', 'dropout', 'transformer_layers_per_block', 'cross_attention_norm', 'projection_class_embeddings_input_dim', 'attention_type', 'resnet_out_scale_factor', 'conv_out_kernel', 'addition_embed_type', 'reverse_transformer_layers_per_block', 'addition_time_embed_dim', 'time_embedding_dim', 'class_embed_type', 'resnet_skip_time_act', 'addition_embed_type_num_heads', 'class_embeddings_concat', 'timestep_post_act', 'resnet_time_scale_shift'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stabilityai/stable-diffusion-2-1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "INFO:__main__:***** Running training *****\n",
      "INFO:__main__:  Num examples = 10805\n",
      "INFO:__main__:  Num Epochs = 1\n",
      "INFO:__main__:  Instantaneous batch size per device = 8\n",
      "INFO:__main__:  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "INFO:__main__:  Gradient Accumulation steps = 4\n",
      "INFO:__main__:  Total optimization steps = 10\n",
      "\n",
      "Steps:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Steps:   0%|          | 0/10 [00:09<?, ?it/s, lr=1e-5, step_loss=0.252]\n",
      "Steps:   0%|          | 0/10 [00:11<?, ?it/s, lr=1e-5, step_loss=0.316]\n",
      "Steps:   0%|          | 0/10 [00:13<?, ?it/s, lr=1e-5, step_loss=0.268]\n",
      "Steps:  10%|█         | 1/10 [00:15<02:19, 15.49s/it, lr=1e-5, step_loss=0.268]\n",
      "Steps:  10%|█         | 1/10 [00:21<02:19, 15.49s/it, lr=9e-6, step_loss=0.269]\n",
      "Steps:  10%|█         | 1/10 [00:23<02:19, 15.49s/it, lr=9e-6, step_loss=0.281]\n",
      "Steps:  10%|█         | 1/10 [00:25<02:19, 15.49s/it, lr=9e-6, step_loss=0.214]\n",
      "Steps:  10%|█         | 1/10 [00:28<02:19, 15.49s/it, lr=9e-6, step_loss=0.268]\n",
      "Steps:  20%|██        | 2/10 [00:30<01:59, 14.99s/it, lr=9e-6, step_loss=0.268]\n",
      "Steps:  20%|██        | 2/10 [00:35<01:59, 14.99s/it, lr=8e-6, step_loss=0.271]\n",
      "Steps:  20%|██        | 2/10 [00:37<01:59, 14.99s/it, lr=8e-6, step_loss=0.234]\n",
      "Steps:  20%|██        | 2/10 [02:00<01:59, 14.99s/it, lr=8e-6, step_loss=0.274]\n",
      "Steps:  20%|██        | 2/10 [02:02<01:59, 14.99s/it, lr=8e-6, step_loss=0.195]\n",
      "Steps:  30%|███       | 3/10 [02:04<05:58, 51.25s/it, lr=8e-6, step_loss=0.195]\n",
      "Steps:  30%|███       | 3/10 [02:04<05:58, 51.25s/it, lr=7e-6, step_loss=0.225]\n",
      "Steps:  30%|███       | 3/10 [02:06<05:58, 51.25s/it, lr=7e-6, step_loss=0.279]\n",
      "Steps:  30%|███       | 3/10 [02:08<05:58, 51.25s/it, lr=7e-6, step_loss=0.276]\n",
      "Steps:  30%|███       | 3/10 [02:10<05:58, 51.25s/it, lr=7e-6, step_loss=0.309]\n",
      "Steps:  40%|████      | 4/10 [02:13<03:26, 34.39s/it, lr=7e-6, step_loss=0.309]\n",
      "Steps:  40%|████      | 4/10 [02:13<03:26, 34.39s/it, lr=6e-6, step_loss=0.252]\n",
      "Steps:  40%|████      | 4/10 [02:15<03:26, 34.39s/it, lr=6e-6, step_loss=0.243]\n",
      "Steps:  40%|████      | 4/10 [02:17<03:26, 34.39s/it, lr=6e-6, step_loss=0.266]\n",
      "Steps:  40%|████      | 4/10 [02:19<03:26, 34.39s/it, lr=6e-6, step_loss=0.223]\n",
      "Steps:  50%|█████     | 5/10 [02:21<02:05, 25.13s/it, lr=6e-6, step_loss=0.223]\n",
      "Steps:  50%|█████     | 5/10 [02:21<02:05, 25.13s/it, lr=5e-6, step_loss=0.228]\n",
      "Steps:  50%|█████     | 5/10 [02:23<02:05, 25.13s/it, lr=5e-6, step_loss=0.306]\n",
      "Steps:  50%|█████     | 5/10 [02:26<02:05, 25.13s/it, lr=5e-6, step_loss=0.27] \n",
      "Steps:  50%|█████     | 5/10 [02:28<02:05, 25.13s/it, lr=5e-6, step_loss=0.262]\n",
      "Steps:  60%|██████    | 6/10 [02:30<01:18, 19.56s/it, lr=5e-6, step_loss=0.262]\n",
      "Steps:  60%|██████    | 6/10 [02:30<01:18, 19.56s/it, lr=4e-6, step_loss=0.231]\n",
      "Steps:  60%|██████    | 6/10 [02:32<01:18, 19.56s/it, lr=4e-6, step_loss=0.295]\n",
      "Steps:  60%|██████    | 6/10 [02:34<01:18, 19.56s/it, lr=4e-6, step_loss=0.224]\n",
      "Steps:  60%|██████    | 6/10 [02:36<01:18, 19.56s/it, lr=4e-6, step_loss=0.281]\n",
      "Steps:  70%|███████   | 7/10 [02:39<00:47, 15.97s/it, lr=4e-6, step_loss=0.281]\n",
      "Steps:  70%|███████   | 7/10 [02:39<00:47, 15.97s/it, lr=3e-6, step_loss=0.246]\n",
      "Steps:  70%|███████   | 7/10 [02:41<00:47, 15.97s/it, lr=3e-6, step_loss=0.234]\n",
      "Steps:  70%|███████   | 7/10 [02:43<00:47, 15.97s/it, lr=3e-6, step_loss=0.308]\n",
      "Steps:  70%|███████   | 7/10 [02:45<00:47, 15.97s/it, lr=3e-6, step_loss=0.262]\n",
      "Steps:  80%|████████  | 8/10 [02:47<00:27, 13.58s/it, lr=3e-6, step_loss=0.262]\n",
      "Steps:  80%|████████  | 8/10 [02:47<00:27, 13.58s/it, lr=2e-6, step_loss=0.222]\n",
      "Steps:  80%|████████  | 8/10 [02:49<00:27, 13.58s/it, lr=2e-6, step_loss=0.293]\n",
      "Steps:  80%|████████  | 8/10 [02:51<00:27, 13.58s/it, lr=2e-6, step_loss=0.272]\n",
      "Steps:  80%|████████  | 8/10 [02:53<00:27, 13.58s/it, lr=2e-6, step_loss=0.227]\n",
      "Steps:  90%|█████████ | 9/10 [02:55<00:11, 11.96s/it, lr=2e-6, step_loss=0.227]\n",
      "Steps:  90%|█████████ | 9/10 [02:55<00:11, 11.96s/it, lr=1e-6, step_loss=0.268]\n",
      "Steps:  90%|█████████ | 9/10 [02:58<00:11, 11.96s/it, lr=1e-6, step_loss=0.231]\n",
      "Steps:  90%|█████████ | 9/10 [03:00<00:11, 11.96s/it, lr=1e-6, step_loss=0.217]\n",
      "Steps:  90%|█████████ | 9/10 [03:02<00:11, 11.96s/it, lr=1e-6, step_loss=0.259]\n",
      "Steps: 100%|██████████| 10/10 [03:04<00:00, 10.84s/it, lr=1e-6, step_loss=0.259]\n",
      "Steps: 100%|██████████| 10/10 [03:04<00:00, 10.84s/it, lr=0, step_loss=0.264]   {'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "train_loss: 0.276275\n",
      "train_loss: 0.258689\n",
      "train_loss: 0.232012\n",
      "train_loss: 0.278887\n",
      "train_loss: 0.240383\n",
      "train_loss: 0.267269\n",
      "train_loss: 0.261556\n",
      "train_loss: 0.256176\n",
      "train_loss: 0.265017\n",
      "train_loss: 0.242792\n",
      "\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  17%|█▋        | 1/6 [00:00<00:00,  8.06it/s]\u001b[AInstantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'mid_block_add_attention', 'shift_factor', 'force_upcast', 'use_post_quant_conv', 'scaling_factor', 'latents_std', 'latents_mean', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  33%|███▎      | 2/6 [00:01<00:02,  1.41it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  83%|████████▎ | 5/6 [00:03<00:00,  1.28it/s]\u001b[A{'sample_max_value', 'rescale_betas_zero_snr', 'thresholding', 'clip_sample_range', 'timestep_spacing', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:03<00:00,  1.63it/s]\n",
      "INFO:__main__:Running validation... \n",
      " Generating 4 images with prompt: A portrait of a slim white woman in her twenties, plain background..\n",
      "Model weights saved in optuna-sd-model_bs8_lr1e-05_schedlinear_acc4_4/pytorch_lora_weights.safetensors\n",
      "{'image_encoder'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "\n",
      "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Instantiating AutoencoderKL model under default dtype torch.float16.\n",
      "{'mid_block_add_attention', 'shift_factor', 'force_upcast', 'use_post_quant_conv', 'scaling_factor', 'latents_std', 'latents_mean', 'use_quant_conv'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing AutoencoderKL.\n",
      "\n",
      "All the weights of AutoencoderKL were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/vae.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  33%|███▎      | 2/6 [00:00<00:00,  9.34it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "Instantiating UNet2DConditionModel model under default dtype torch.float16.\n",
      "{'conv_in_kernel', 'encoder_hid_dim', 'num_attention_heads', 'mid_block_only_cross_attention', 'time_cond_proj_dim', 'time_embedding_type', 'encoder_hid_dim_type', 'time_embedding_act_fn', 'mid_block_type', 'dropout', 'transformer_layers_per_block', 'cross_attention_norm', 'projection_class_embeddings_input_dim', 'attention_type', 'resnet_out_scale_factor', 'conv_out_kernel', 'addition_embed_type', 'reverse_transformer_layers_per_block', 'addition_time_embed_dim', 'time_embedding_dim', 'class_embed_type', 'resnet_skip_time_act', 'addition_embed_type_num_heads', 'class_embeddings_concat', 'timestep_post_act', 'resnet_time_scale_shift'} was not found in config. Values will be initialized to default values.\n",
      "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
      "\n",
      "All the weights of UNet2DConditionModel were initialized from the model checkpoint at /root/.cache/huggingface/hub/models--stabilityai--stable-diffusion-2-1/snapshots/5cae40e6a2745ae2b01ad92ae5043f95f23644d6/unet.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  67%|██████▋   | 4/6 [00:16<00:09,  4.93s/it]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "\n",
      "Loading pipeline components...:  83%|████████▎ | 5/6 [00:18<00:03,  3.95s/it]\u001b[A{'sample_max_value', 'rescale_betas_zero_snr', 'thresholding', 'clip_sample_range', 'timestep_spacing', 'dynamic_thresholding_ratio'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as DDIMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1.\n",
      "\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:18<00:00,  3.07s/it]\n",
      "Loading unet.\n",
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n",
      "INFO:__main__:Running validation... \n",
      " Generating 4 images with prompt: A portrait of a slim white woman in her twenties, plain background..\n",
      "\n",
      "Steps: 100%|██████████| 10/10 [05:09<00:00, 30.99s/it, lr=0, step_loss=0.264]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-15 17:33:14,280] Trial 4 finished with value: 0.276275 and parameters: {'learning_rate': 1e-05, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4}. Best is trial 0 with value: 0.26749.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 4] train_loss: 0.276275\n",
      "0.276275\n",
      "\n",
      "=============================\n",
      "💡 Best trial:\n",
      "FrozenTrial(number=0, state=1, values=[0.26749], datetime_start=datetime.datetime(2025, 5, 15, 16, 58, 47, 613194), datetime_complete=datetime.datetime(2025, 5, 15, 17, 6, 37, 442470), params={'learning_rate': 5e-05, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': CategoricalDistribution(choices=(1e-05, 5e-05, 0.0001, 1e-06)), 'train_batch_size': CategoricalDistribution(choices=(8,)), 'lr_scheduler': CategoricalDistribution(choices=('cosine', 'linear')), 'gradient_accumulation_steps': CategoricalDistribution(choices=(4, 8))}, trial_id=0, value=None)\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import subprocess\n",
    "import joblib\n",
    "import optuna.visualization as vis\n",
    "from datetime import datetime\n",
    "import sys\n",
    "# ========== OBJECTIVE FUNCTION ==========\n",
    "def objective(trial):\n",
    "    # Hyperparamètres à optimiser\n",
    "    learning_rate = trial.suggest_categorical(\"learning_rate\", [1e-5, 5e-5, 1e-4, 1e-6])\n",
    "    batch_size = trial.suggest_categorical(\"train_batch_size\", [8])\n",
    "    lr_scheduler = trial.suggest_categorical(\"lr_scheduler\", [\"cosine\", \"linear\"])\n",
    "    gradient_accumulation = trial.suggest_categorical(\"gradient_accumulation_steps\", [4, 8])\n",
    "\n",
    "    # Nom unique pour chaque essai\n",
    "    output_dir = f\"optuna-sd-model_bs{batch_size}_lr{learning_rate}_sched{lr_scheduler}_acc{gradient_accumulation}_{trial.number}\"\n",
    "\n",
    "    # Commande d'entraînement\n",
    "    command = [\n",
    "        \"accelerate\", \"launch\", \"train_text_to_image_lora.py\",\n",
    "        \"--pretrained_model_name_or_path=stabilityai/stable-diffusion-2-1\",\n",
    "        \"--train_data_dir=/content/drive/MyDrive/PFE/main/PFE/dataset/images\",\n",
    "        \"--caption_column=prompt\",\n",
    "        \"--dataloader_num_workers=8\",\n",
    "        \"--resolution=512\",\n",
    "        f\"--train_batch_size={batch_size}\",\n",
    "        \"--max_train_steps=10\",\n",
    "        \"--max_grad_norm=1\",\n",
    "        f\"--gradient_accumulation_steps={gradient_accumulation}\",\n",
    "        \"--num_train_epochs=2\",\n",
    "        \"--checkpointing_steps=150\",\n",
    "        f\"--learning_rate={learning_rate}\",\n",
    "        f\"--lr_scheduler={lr_scheduler}\",\n",
    "        \"--lr_warmup_steps=0\",\n",
    "        \"--mixed_precision=fp16\",\n",
    "        \"--seed=1337\",\n",
    "        '--allow_tf32',\n",
    "        '--use_8bit_adam',\n",
    "        f\"--output_dir={output_dir}\",\n",
    "        \"--validation_prompt=A portrait of a slim white woman in her twenties, plain background.\"\n",
    "\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n🚀 [Trial {trial.number}] Commande:\\n{' '.join(command)}\\n\")\n",
    "\n",
    "    # Exécution avec affichage direct\n",
    "    #---------------------------------------\n",
    "\n",
    "    import shlex\n",
    "    process = subprocess.Popen(\n",
    "    command,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    text=True,\n",
    "    bufsize=1\n",
    "    )\n",
    "\n",
    "    log_lines = []\n",
    "    for line in process.stdout:\n",
    "      print(line, end=\"\")  # Affiche chaque ligne en temps réel\n",
    "      log_lines.append(line)\n",
    "\n",
    "    process.wait()\n",
    "    output_text = \"\".join(log_lines)\n",
    "\n",
    "    #result = subprocess.run(command, capture_output=True, text=True)\n",
    "    #print(result.stderr)\n",
    "    #print('-'*30)\n",
    "    #print(result.stdout)\n",
    "    # Extraction de la perte depuis stdout\n",
    "    if \"train_loss\" in output_text:\n",
    "        for line in output_text.splitlines():\n",
    "            if \"train_loss\" in line:\n",
    "                try:\n",
    "                    loss = float(line.split(\"train_loss\")[1].split()[1])\n",
    "                    print(f\"[Trial {trial.number}] train_loss: {loss}\")\n",
    "                    print(loss)\n",
    "                    return loss\n",
    "                except Exception as e:\n",
    "                    print(f\"[Trial {trial.number}] Erreur extraction train_loss:\", e)\n",
    "\n",
    "    return float(\"inf\")  # Mauvais essai si échec\n",
    "\n",
    "\n",
    "# ========== LANCER L’OPTIMISATION ==========\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "study_name = f\"sd_optuna_study_{timestamp}\"\n",
    "study = optuna.create_study(direction=\"minimize\", study_name=study_name)\n",
    "study.optimize(objective, n_trials=5)\n",
    "# Sauvegarde du modèle d’étude\n",
    "joblib.dump(study, f\"{study_name}.pkl\")\n",
    "print(\"\\n=============================\")\n",
    "print(\"💡 Best trial:\")\n",
    "print(study.best_trial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2331,
     "status": "ok",
     "timestamp": 1747330436650,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "dN7cmqOHHtNd",
    "outputId": "c8b3c660-8e5c-4e68-fcc4-87482043bcb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Génération des graphiques Optuna...\n",
      "[FrozenTrial(number=0, state=1, values=[0.26749], datetime_start=datetime.datetime(2025, 5, 15, 16, 58, 47, 613194), datetime_complete=datetime.datetime(2025, 5, 15, 17, 6, 37, 442470), params={'learning_rate': 5e-05, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': CategoricalDistribution(choices=(1e-05, 5e-05, 0.0001, 1e-06)), 'train_batch_size': CategoricalDistribution(choices=(8,)), 'lr_scheduler': CategoricalDistribution(choices=('cosine', 'linear')), 'gradient_accumulation_steps': CategoricalDistribution(choices=(4, 8))}, trial_id=0, value=None), FrozenTrial(number=1, state=1, values=[0.276275], datetime_start=datetime.datetime(2025, 5, 15, 17, 6, 37, 443594), datetime_complete=datetime.datetime(2025, 5, 15, 17, 13, 14, 516220), params={'learning_rate': 0.0001, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': CategoricalDistribution(choices=(1e-05, 5e-05, 0.0001, 1e-06)), 'train_batch_size': CategoricalDistribution(choices=(8,)), 'lr_scheduler': CategoricalDistribution(choices=('cosine', 'linear')), 'gradient_accumulation_steps': CategoricalDistribution(choices=(4, 8))}, trial_id=1, value=None), FrozenTrial(number=2, state=1, values=[0.26749], datetime_start=datetime.datetime(2025, 5, 15, 17, 13, 14, 517463), datetime_complete=datetime.datetime(2025, 5, 15, 17, 21, 0, 565091), params={'learning_rate': 5e-05, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 8}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': CategoricalDistribution(choices=(1e-05, 5e-05, 0.0001, 1e-06)), 'train_batch_size': CategoricalDistribution(choices=(8,)), 'lr_scheduler': CategoricalDistribution(choices=('cosine', 'linear')), 'gradient_accumulation_steps': CategoricalDistribution(choices=(4, 8))}, trial_id=2, value=None), FrozenTrial(number=3, state=1, values=[0.276275], datetime_start=datetime.datetime(2025, 5, 15, 17, 21, 0, 566413), datetime_complete=datetime.datetime(2025, 5, 15, 17, 27, 0, 845880), params={'learning_rate': 5e-05, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': CategoricalDistribution(choices=(1e-05, 5e-05, 0.0001, 1e-06)), 'train_batch_size': CategoricalDistribution(choices=(8,)), 'lr_scheduler': CategoricalDistribution(choices=('cosine', 'linear')), 'gradient_accumulation_steps': CategoricalDistribution(choices=(4, 8))}, trial_id=3, value=None), FrozenTrial(number=4, state=1, values=[0.276275], datetime_start=datetime.datetime(2025, 5, 15, 17, 27, 0, 847608), datetime_complete=datetime.datetime(2025, 5, 15, 17, 33, 14, 279766), params={'learning_rate': 1e-05, 'train_batch_size': 8, 'lr_scheduler': 'linear', 'gradient_accumulation_steps': 4}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'learning_rate': CategoricalDistribution(choices=(1e-05, 5e-05, 0.0001, 1e-06)), 'train_batch_size': CategoricalDistribution(choices=(8,)), 'lr_scheduler': CategoricalDistribution(choices=('cosine', 'linear')), 'gradient_accumulation_steps': CategoricalDistribution(choices=(4, 8))}, trial_id=4, value=None)]\n",
      "✅ Plots sauvegardés en fichiers HTML.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(\"📊 Génération des graphiques Optuna...\")\n",
    "\n",
    "# Ne garder que les essais réussis\n",
    "completed_trials = [t for t in study.trials if t.value is not None and math.isfinite(t.value)]\n",
    "print(completed_trials)\n",
    "if len(completed_trials) < 2:\n",
    "    print(\"⚠️ Pas assez de résultats valides pour les visualisations avancées.\")\n",
    "    vis.plot_optimization_history(study).write_html(\"plot_optimization_history.html\", include_plotlyjs=\"cdn\")\n",
    "else:\n",
    "    vis.plot_optimization_history(study).write_html(\"plot_optimization_history.html\", include_plotlyjs=\"cdn\")\n",
    "    vis.plot_param_importances(study).write_html(\"plot_param_importance.html\", include_plotlyjs=\"cdn\")\n",
    "    vis.plot_parallel_coordinate(study).write_html(\"plot_parallel_coordinates.html\", include_plotlyjs=\"cdn\")\n",
    "    vis.plot_slice(study).write_html(\"plot_slice.html\", include_plotlyjs=\"cdn\")\n",
    "    vis.plot_contour(study, params=[\"learning_rate\", \"gradient_accumulation_steps\"]).write_html(\"plot_contour.html\", include_plotlyjs=\"cdn\")\n",
    "\n",
    "print(\"✅ Plots sauvegardés en fichiers HTML.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118,
     "referenced_widgets": [
      "425fd89fd61349b4b9523bca5ff28852",
      "bcc895cbeb3148c7b999688dc922f893",
      "e6b2bcd071fe424a876cbb27ddb0db2b",
      "fcba60321ea541688822e997f07a00df",
      "d37e4197b96340ca8fe34fc760cc5996",
      "84afc24e75b84cc39e27debc48997f05",
      "a090e99957c14ef591898845b19b126f",
      "63eda89c45c0480bb049bbe7109a5426",
      "ffa3f436a88d4fa3ae89685d08c7e1f7",
      "b3789dee539c4de295f1be955b95e03e",
      "cf6e7fef821c43d28c6fa6937e0ed9c9",
      "3a6ea9ed47f346388c815d1efce53f10",
      "a659f9bac0f34d8eb7daca01b7f265e7",
      "8ae9b29b65434e928462a54c603431d5",
      "d47423d0ce6140429c787ea878535049",
      "e4d3138f4ae446e9839690bdf9d870a6",
      "c7df59da83c143bd8100109b785fdce7",
      "1b38d39289744953b0c59748f5da7eba",
      "fb222cbb942747139701d7d39fdfdcd6",
      "9167a111738e4e4eb1e1c9e42f36282d",
      "a69ae5f5411e4a0d8771114a0a3817fa",
      "ea91a644a3b74340a374d635e7152611"
     ]
    },
    "executionInfo": {
     "elapsed": 38739,
     "status": "ok",
     "timestamp": 1747330990946,
     "user": {
      "displayName": "Solution 30",
      "userId": "15221712578988587613"
     },
     "user_tz": -60
    },
    "id": "LtDhy2Nepm-8",
    "outputId": "327429e7-7509-468e-f331-d856673ef3cc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425fd89fd61349b4b9523bca5ff28852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No LoRA keys associated to CLIPTextModel found with the prefix='text_encoder'. This is safe to ignore if LoRA state dict didn't originally have any CLIPTextModel related params. You can also try specifying `prefix=None` to resolve the warning. Otherwise, open an issue if you think it's unexpected: https://github.com/huggingface/diffusers/issues/new\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6ea9ed47f346388c815d1efce53f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_path = \"/content/drive/MyDrive/PFE/main/PFE/models/diffusers/examples/text_to_image/optuna-sd-model_bs8_lr1e-05_schedlinear_acc8_0\"\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16)\n",
    "pipe.to(\"cuda\")\n",
    "pipe.load_lora_weights(model_path)\n",
    "\n",
    "prompt = \"A portrait of a pregnant white woman in her twenties, plain background.\"\n",
    "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
    "image.save(\"/content/img.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYqq8M5SCHQo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "e7DL-5T6a7y2"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ab709fc478c47ad9b4b32835c08b7f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b6f75c73b4245e5ac011b79ff88e316": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f98e3ac88de4c0a8562509fdddf2414": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce2c638c0b2c425f85992f4cb3ae7914",
      "placeholder": "​",
      "style": "IPY_MODEL_6e213250555c4fae8e8b4e12e4ac528b",
      "value": " 5/5 [00:22&lt;00:00,  4.99s/it]"
     }
    },
    "18f1b6809eb7462eb8992b0ef123ed54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_ea9a99ce48ca40b5955ba6bf981b8904",
      "style": "IPY_MODEL_60af33f0b5b245ddbfdf1b6048dce0ce",
      "tooltip": ""
     }
    },
    "1b38d39289744953b0c59748f5da7eba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24581c35eccb4fcaa24783a372c1369b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d83a0171e7242eabcd9b48ede179ab8",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5160ab7f0ea4475b88c3f22befad8c9f",
      "value": 5
     }
    },
    "30ec6de5c21f4d57b04352fa516c1894": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a6ea9ed47f346388c815d1efce53f10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a659f9bac0f34d8eb7daca01b7f265e7",
       "IPY_MODEL_8ae9b29b65434e928462a54c603431d5",
       "IPY_MODEL_d47423d0ce6140429c787ea878535049"
      ],
      "layout": "IPY_MODEL_e4d3138f4ae446e9839690bdf9d870a6"
     }
    },
    "3de95f399ca14a6b87596df4f19401fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4801fdbb01e54152baf472fca2d45ca4",
      "placeholder": "​",
      "style": "IPY_MODEL_63748aebd8224662b91987cf59a7d239",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "404b03575c404e0caa036ed4ca71713a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "425fd89fd61349b4b9523bca5ff28852": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bcc895cbeb3148c7b999688dc922f893",
       "IPY_MODEL_e6b2bcd071fe424a876cbb27ddb0db2b",
       "IPY_MODEL_fcba60321ea541688822e997f07a00df"
      ],
      "layout": "IPY_MODEL_d37e4197b96340ca8fe34fc760cc5996"
     }
    },
    "47c33d1b613d4020bd95daf95614605c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e972ff5985dc45e2b55c06a54614eb73",
       "IPY_MODEL_24581c35eccb4fcaa24783a372c1369b",
       "IPY_MODEL_0f98e3ac88de4c0a8562509fdddf2414"
      ],
      "layout": "IPY_MODEL_b4f7c1eb7e714503997750d7491e09df"
     }
    },
    "4801fdbb01e54152baf472fca2d45ca4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "486b60aed1e248a59089d373e09255c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5160ab7f0ea4475b88c3f22befad8c9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5745d5f89277404996948f96880dd048": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5aaf113e6c274350927dc7ffba2fd29a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "60af33f0b5b245ddbfdf1b6048dce0ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "63748aebd8224662b91987cf59a7d239": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63eda89c45c0480bb049bbe7109a5426": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ceca2f4addf4d6abe4c7c2e9cec59f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2a12f9e346d4e6294df808c7a590e13",
      "placeholder": "​",
      "style": "IPY_MODEL_30ec6de5c21f4d57b04352fa516c1894",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "6e213250555c4fae8e8b4e12e4ac528b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "728750cf0d6240d7b986a90004dbd31b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "83571e554b864995bab0f73ff6bf38e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "84afc24e75b84cc39e27debc48997f05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87ef2ee51ab0490bae86566a5d429e2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ae9b29b65434e928462a54c603431d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb222cbb942747139701d7d39fdfdcd6",
      "max": 30,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9167a111738e4e4eb1e1c9e42f36282d",
      "value": 30
     }
    },
    "8d83a0171e7242eabcd9b48ede179ab8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9167a111738e4e4eb1e1c9e42f36282d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "995e0ef750e9461b9ad8c025aa498b83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5745d5f89277404996948f96880dd048",
      "max": 30,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83571e554b864995bab0f73ff6bf38e7",
      "value": 30
     }
    },
    "a06b890f9f944c77a9f3c885290495d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "a090e99957c14ef591898845b19b126f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a659f9bac0f34d8eb7daca01b7f265e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7df59da83c143bd8100109b785fdce7",
      "placeholder": "​",
      "style": "IPY_MODEL_1b38d39289744953b0c59748f5da7eba",
      "value": "100%"
     }
    },
    "a69ae5f5411e4a0d8771114a0a3817fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab0b32dcd5d64ac79370f8322a9dab28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_87ef2ee51ab0490bae86566a5d429e2a",
      "style": "IPY_MODEL_728750cf0d6240d7b986a90004dbd31b",
      "value": true
     }
    },
    "ab9a6d775efd4181ba57cbc887386b79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e60e1220d27b4bc89c765736c18bbb6a",
      "placeholder": "​",
      "style": "IPY_MODEL_404b03575c404e0caa036ed4ca71713a",
      "value": "Connecting..."
     }
    },
    "b3789dee539c4de295f1be955b95e03e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4f7c1eb7e714503997750d7491e09df": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6a01add170f4a81a2fe824c4f1e3e7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bcc895cbeb3148c7b999688dc922f893": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_84afc24e75b84cc39e27debc48997f05",
      "placeholder": "​",
      "style": "IPY_MODEL_a090e99957c14ef591898845b19b126f",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "bd57424576d2425185e34ab3464f7ee1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [],
      "layout": "IPY_MODEL_a06b890f9f944c77a9f3c885290495d0"
     }
    },
    "c16c7cdda7d14befade3967c3c5fea1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c901c86260be47ad965ae46a7cddaad0",
       "IPY_MODEL_995e0ef750e9461b9ad8c025aa498b83",
       "IPY_MODEL_fd5a2c5a60cb4c52bce596930902fd18"
      ],
      "layout": "IPY_MODEL_0ab709fc478c47ad9b4b32835c08b7f2"
     }
    },
    "c7df59da83c143bd8100109b785fdce7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c901c86260be47ad965ae46a7cddaad0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_486b60aed1e248a59089d373e09255c1",
      "placeholder": "​",
      "style": "IPY_MODEL_0b6f75c73b4245e5ac011b79ff88e316",
      "value": "100%"
     }
    },
    "ca1e5c8ddbee49b78af11138dc4c89f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_b6a01add170f4a81a2fe824c4f1e3e7a",
      "placeholder": "​",
      "style": "IPY_MODEL_cd56972823b2436aa56496e040746628",
      "value": ""
     }
    },
    "cd56972823b2436aa56496e040746628": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ce1566c5373f45eb8ef29d39506fd191": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce2c638c0b2c425f85992f4cb3ae7914": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf6e7fef821c43d28c6fa6937e0ed9c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d37e4197b96340ca8fe34fc760cc5996": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d47423d0ce6140429c787ea878535049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a69ae5f5411e4a0d8771114a0a3817fa",
      "placeholder": "​",
      "style": "IPY_MODEL_ea91a644a3b74340a374d635e7152611",
      "value": " 30/30 [00:09&lt;00:00,  3.10it/s]"
     }
    },
    "e4d3138f4ae446e9839690bdf9d870a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e60e1220d27b4bc89c765736c18bbb6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6b2bcd071fe424a876cbb27ddb0db2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63eda89c45c0480bb049bbe7109a5426",
      "max": 6,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ffa3f436a88d4fa3ae89685d08c7e1f7",
      "value": 6
     }
    },
    "e972ff5985dc45e2b55c06a54614eb73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f47947a165e3431faf026311014037ca",
      "placeholder": "​",
      "style": "IPY_MODEL_5aaf113e6c274350927dc7ffba2fd29a",
      "value": "Loading pipeline components...: 100%"
     }
    },
    "ea4b1c230fa94020bc9039b5d82f2c40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea91a644a3b74340a374d635e7152611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea9a99ce48ca40b5955ba6bf981b8904": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2a12f9e346d4e6294df808c7a590e13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f47947a165e3431faf026311014037ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb222cbb942747139701d7d39fdfdcd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcba60321ea541688822e997f07a00df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3789dee539c4de295f1be955b95e03e",
      "placeholder": "​",
      "style": "IPY_MODEL_cf6e7fef821c43d28c6fa6937e0ed9c9",
      "value": " 6/6 [00:25&lt;00:00,  6.19s/it]"
     }
    },
    "fd5a2c5a60cb4c52bce596930902fd18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce1566c5373f45eb8ef29d39506fd191",
      "placeholder": "​",
      "style": "IPY_MODEL_ea4b1c230fa94020bc9039b5d82f2c40",
      "value": " 30/30 [00:04&lt;00:00,  6.13it/s]"
     }
    },
    "ffa3f436a88d4fa3ae89685d08c7e1f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
